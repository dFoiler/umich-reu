% !TEX root = ../intertwining.tex

\section{Group-Theoretic Set-Up} \label{sec:rep-theory}
In this section, we set up the necessary representation theory to proceed with the results in the rest of the paper.

\subsection{Groups and Subgroups}
Let $q$ be an odd prime-power, and let $2n$ be a positive even integer; for convenience, we will take $3\nmid q$, but this is used infrequently. Throughout, $G$ will be one of the groups $\{{\GL}_{2n},{\SL}_{2n},\GO_{2n},\O_{2n},\GSp_{2n},\Sp_{2n}\}$ over the finite field $\FF_q$. To explicate our orthogonal and symplectic groups, we fix
\[\varepsilon\coloneqq\begin{cases}
    +1 & \text{if }G\in\{\GO_{2n},\O_{2n}\}, \\
    -1 & \text{if }G\in\{\GSp_{2n},\Sp_{2n}\},
\end{cases}\qquad\text{and}\qquad J\coloneqq\begin{bmatrix}
    & \varepsilon1_n \\
    1_n
\end{bmatrix}\]
so that $G$ is defined to preserve the quadratic form $J$. In the cases where $G\in\{\GL_{2n},\SL_{2n}\}$, it will be convenient to define $\varepsilon\coloneqq-1$ as well. Here, the blank entries in $J$ indicate zeroes, a convention that will stay in place for the rest of the article. Throughout, when there are multiple groups $G$ involved, we will use a superscript $(\cdot)^G$ to distinguish between multiple elements; for example, $\varepsilon^{\GL_{2n}}=-1$.

Note that $G$ has split maximal torus $T$ given by the diagonal matrices. Anyway, the benefit $2n$ being even is that we may use the Siegel parabolic subgroup
\[P\coloneqq\left\{\begin{bmatrix}
    A & B \\
      & D
\end{bmatrix}\in G\right\},\]
where $A,B,C,D$ are implicitly in $\FF_q^{n\times n}$, a convention that will remain in place for any expression in block matrix form as above. We let $U\subseteq P$ be the unipotent radical of $P$, and we let $M\subseteq P$ be the Levi subgroup so that $P=M\ltimes U$. Explicitly,
\[U=\left\{\begin{bmatrix}
    1_n & B \\
      & 1_n
\end{bmatrix}\in G\right\}\qquad\text{and}\qquad M=\left\{\begin{bmatrix}
    A &   \\
      & D
\end{bmatrix}\in G\right\}.\]
The various cases of $G$ provide more constraints on these subgroups. For example, if $G\in\{\GO_{2n},\O_{2n}\}$, then $B$ above must be alternating; if $G\in\{\GSp_{2n},\Sp_{2n}\}$, then $B$ above must be symmetric. Similarly, if $G=\SL_{2n}$, then $\det D=(\det A)^{-1}$; if $G\in\{\O_{2n},\Sp_{2n}\}$, then $D=A^{-\intercal}$; and if $G\in\{\GO_{2n},\GSp_{2n}\}$, then $D=\lambda A^{-\intercal}$ for some $\lambda\in\FF_q^\times$. A quick computation with the definition of $G$ in the various cases reveals that these are only the constraints.

It will be helpful in the sequel to understand characters of $P$. In all cases, we are able to define a ``Siegel determinant'' $\chi_{\det}\colon P\to\FF_q^\times$ given by
\[\chi_{\det}\left(\begin{bmatrix}
    A & B \\
      & D
\end{bmatrix}\right)\coloneqq(\det D)^{-1}.\]
In the cases $G\in\{\GL_{2n},\GO_{2n},\GSp_{2n}\}$, there is an additional ``multiplier'' $m\colon P\to\FF_q^\times$ given by
\[m\left(\begin{bmatrix}
    A & B \\
      & D
\end{bmatrix}\right)=\det AD\text{ if }G=\GL_{2n},\text{ and }m\left(\begin{bmatrix}
    \lambda A & B \\
      & A^{-\intercal}
\end{bmatrix}\right)\coloneqq\lambda\text{ else}.\]
For the remaining cases of $G$, we will define $m$ to just be the trivial character. Both $\chi_{\det}$ and $m$ are characters by a direct computation. It turns out that these are essentially the only characters.
\begin{lemma} \label{lem:decompose-character}
    Let $\chi\colon P\to\CC$ be a character. Then $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$ for some characters $\alpha,\beta\colon\FF_q^\times\to\CC$.
\end{lemma}
\begin{proof}
    We will do casework on $G$, but before going any further, we argue (directly) that $\chi$ vanishes on $U$; we will actually show that $U$ is contained in the commutator subgroup $[P,P]$. Well, choose some $B$ such that $u\coloneqq\begin{bsmallmatrix}
        1_n & B \\
        & 1_n
    \end{bsmallmatrix}$ is in $U$. Choosing some $a\in\FF_q^\times$, we note $\begin{bsmallmatrix}
        a1_n \\ & a^{-1}1_n
    \end{bsmallmatrix}\in M$, so we consider the commutator
    \[\begin{bmatrix}
        a1_n \\ & a^{-1}1_n
    \end{bmatrix}\begin{bmatrix}
        1_n & B \\ & 1_n
    \end{bmatrix}\begin{bmatrix}
        a1_n \\ & a^{-1}1_n
    \end{bmatrix}^{-1}\begin{bmatrix}
        1_n & B \\ & 1_n
    \end{bmatrix}^{-1}=\begin{bmatrix}
        1_n & \left(a^2-1\right)B \\ & 1_n
    \end{bmatrix}.\]
    As long as we can choose $a$ such that $a^2-1\in\FF_q^\times$, we can replace $B$ with $\left(a^2-1\right)^{-1}B$ in the above computation to conclude that $u\in[P,P]$. Well, because $3\nmid q$, we may choose $a\coloneqq2$ so that $a^2-1=3$.

    The point of the previous paragraph is that $\chi$ now factors through $P/U=M$, so we may as well consider $\chi$ as a character on $M$. We now proceed with our casework.
    \begin{itemize}
        \item If $G\in\{\GL_{2n},\SL_{2n}\}$, we claim that $\chi$ is also trivial on the subgroup
        \[\left\{\begin{bmatrix}
            A &   \\
              & D
        \end{bmatrix}:\det A=\det D=1\right\}.\]
        Well, fix any such $\begin{bsmallmatrix}
            A \\ & D
        \end{bsmallmatrix}$. Indeed, we are given that $A,D\in\SL_{n}$, so we appeal to the fact that $[\SL_{n},\SL_{n}]=\SL_{n}$ for our $q>3$. Thus, $A$ and $D$ can be expressed as commutators $A=A_1A_2A_1^{-1}A_2^{-1}$ and $D=D_1D_2D_1^{-1}D_2^{-1}$ so that 
        \[\begin{bmatrix}
            A \\ & D
        \end{bmatrix}=\begin{bmatrix}
            A_1 \\ & D_1
        \end{bmatrix}\begin{bmatrix}
            A_2 \\ & D_2
        \end{bmatrix}\begin{bmatrix}
            A_1 \\ & D_1
        \end{bmatrix}^{-1}\begin{bmatrix}
            A_2 \\ & D_2
        \end{bmatrix}^{-1},\]
        so our element is a commutator.
        
        Thus, $\chi$ on $M$ now factors through $M/(\SL_n\times\SL_n)$. If $G=\GL_{2n}$, this quotient is isomorphic to $\FF_q^\times\times\FF_q^\times$ by $\begin{bsmallmatrix}
            A \\ & D
        \end{bsmallmatrix}\mapsto\left(\det AD,\det D^{-1}\right)$, so $\chi$ indeed factors through a product of $m$ and $\chi_{\det}$. On the other hand, if $G=\SL_{2n}$, this quotient is isomorphic to $\FF_q^\times$ just by $\begin{bsmallmatrix}
            A \\ & D
        \end{bsmallmatrix}\mapsto\det D^{-1}$ because we have the condition $\det AD=1$ already, so we again factor through $\chi_{\det}$.

        \item If $G\in\{\GO_{2n},\O_{2n},\GSp_{2n},\Sp_{2n}\}$, we claim that $\chi$ is also trivial on the subgroup
        \[\left\{\begin{bmatrix}
            A \\ & A^{-\intercal}
        \end{bmatrix}:\det A=1\right\}.\]
        Well, fix any such $\begin{bsmallmatrix}
            A \\ & A^{-\intercal}
        \end{bsmallmatrix}$. Again, we are given that $A\in\SL_n$, so we appeal to the fact that $[\SL_{n},\SL_{n}]=\SL_{n}$ for our $q>3$, meaning we may write $A=A_1A_2A_1^{-1}A_2^{-1}$ so that
        \[\begin{bmatrix}
            A \\ & A^{-\intercal}
        \end{bmatrix}=\begin{bmatrix}
            A_1 \\ & A_1^{-\intercal}
        \end{bmatrix}\begin{bmatrix}
            A_2 \\ & A_2^{-\intercal}
        \end{bmatrix}\begin{bmatrix}
            A_1 \\ & A_1^{-\intercal}
        \end{bmatrix}^{-1}\begin{bmatrix}
            A_2 \\ & A_2^{-\intercal}
        \end{bmatrix}^{-1},\]
        so our element is a commutator.

        Thus, $\chi$ now factors through $M/\SL_n$, where $\SL_n$ is embedded into $M$ as the above subgroup. If $G\in\{\O_{2n},\Sp_{2n}\}$, then elements of $M$ look like $\begin{bsmallmatrix}
            A \\ & A^{-\intercal}
        \end{bsmallmatrix}$, so this quotient is isomorphic to $\FF_q^\times$ via $\chi_{\det}$, meaning $\chi$ indeed factors through $\chi_{\det}$. Otherwise, $G\in\{\GO_{2n},\GSp_{2n}\}$, so elements of $M$ look like $\begin{bsmallmatrix}
            \lambda A \\ & A^{-\intercal}
        \end{bsmallmatrix}$, so this quotient is isomorphic to $\FF_q^\times\times\FF_q^\times$ via $(m,\chi_{\det})$, allowing us to conclude again.
        \qedhere
    \end{itemize}
\end{proof}
The lemma now allows us to take any character $\chi\colon P\to\CC^\times$ and define $\alpha_\chi,\beta_\chi\colon\FF_q^\times\to\CC^\times$ such that $\chi=(\alpha_\chi\circ m)(\beta_\chi\circ\chi_{\det})$. When $m$ is trivial, we will take $\alpha_\chi$ to be trivial as well; otherwise, $m$ and $\chi_{\det}$ are surjective, so $\alpha_\chi$ and $\beta_\chi$ are uniquely determined by $\chi$.

\subsection{Some Weyl Group Computations}
An argument similar to \cite[Example~17.88]{milne-alg-group} verifies that the diagonal subgroup $T$ of $G$ is always a maximal torus; namely, one can check that $C_G(T)=T$. Then an argument similar to \cite[Example~17.42]{milne-alg-group} verifies that $N_G(T)$ consists of permutation matrices (up to torus elements); alternatively, one can study the Weyl group of the relevant root system and then convert this back into permutation matrices by hand. In any case, we let $W$ denote the Weyl group of $G$, and we let $W_P$ denote the Weyl group of the Siegel parabolic subgroup $P$.

It will be useful to explicitly compute these Weyl groups explicitly. If $G\in\{\GL_n,\SL_n\}$, then $W$ consists of the permutation matrices up to a sign. For each $w\in W$, we let $\sigma_w\in N_G(T)$ denote the corresponding permutation matrix, and we let $d_w\in T$ be a diagonal matrix with entries in $\pm1$ such that $\det d_w\sigma_w=1$. (The choice of $d_w$ will not matter too much in the sequel.) The point is that $\{d_w\sigma_w\}_{w\in W}$ provides a set of representatives for $W$ in $G$.

We would like a similar description for $G\in\{\GO_{2n},\O_{2n},\GSp_{2n},\Sp_{2n}\}$. The main point of the computation here is to ask which permutation matrices $\sigma$ actually belong to $G$, and then we can again use $d_w$ to correct for the determinant condition.
\begin{lemma} \label{lem:weyl-normal-form}
    Suppose $G\in\{\GO_{2n},\O_{2n},\GSp_{2n},\Sp_{2n}\}$. Let $\Sigma$ be the set of permutations $\sigma\in S_{2n}$ such that $\sigma(i+n)\equiv\sigma(i)+n\pmod{2n}$ for each $i$.
    \begin{enumerate}[label=(\alph*)]
        \item For each $w$ representing a class in $W$, there exists a unique permutation $\sigma\in\Sigma$ such that $w=d\sigma$ for some diagonal matrix $d$.
        \item For each $\sigma\in\Sigma$, there exists some diagonal matrix $d$ with entries in $\{\pm1\}$ such that $d\sigma\in G$. %In fact, $d=\operatorname{diag}(d_1,d_2,\ldots,d_{2n})$ is uniquely determined by the values $\{d_{\sigma(1)},\ldots,d_{\sigma(n)}\}$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We will show the parts independently.
    \begin{enumerate}[label=(\alph*)]
        \item Recalling that the diagonal matrices of $G$ make up a maximal torus in $B$, we note that diagonal matrices are normalized by the semidirect product of permutation matrices and diagonal matrices (this is even true in $\GL_{2n}$), so we can view elements of $W$ as permutation matrices with elements adjusted by a diagonal element to lie in $G$.
        
        In particular, we may write $w=d\sigma$ for some diagonal matrix $d$, and this $\sigma$ is unique. It remains to show $\sigma\in\Sigma$. Well, the main point is that $d\sigma\in G$ requires
        \[d\sigma J\sigma^\intercal d^\intercal=\lambda J\]
        for some scalar $\lambda$, possibly forced equal to $1$ if $G\in\{\O_{2n},\Sp_{2n}\}$. Setting $d\coloneqq\operatorname{diag}(d_1,\ldots,d_{2n})$, we now pass through a basis vector $e_{\sigma(i)}$ to compute
        \begin{equation}
            \varepsilon^{1_{i>n}}d_{\sigma(i+n)}d_{\sigma(i)}e_{\sigma(i+n)}=\varepsilon^{1_{\sigma(i)>n}}e_{\sigma(i)+n}, \label{eq:test-sigma-in-gsp}
        \end{equation}
        where indices live in $\{1,2,\ldots,2n\}$ but are considered$\pmod{2n}$. Because the diagonal elements of $d$ are nonzero, we must have $\sigma(i+n)\equiv\sigma(i)+n\pmod{2n}$, meaning $\sigma\in\Sigma$.

        \item We need a diagonal matrix $d=\operatorname{diag}(d_1,\ldots,d_{2n})$ such that $d\sigma\in G$, so it is enough for $d\sigma J\sigma^\intercal d^\intercal=J$. Well, it suffices to check this on basis vectors $e_{\sigma(i)}$, for which we see it is enough \eqref{eq:test-sigma-in-gsp}. But because $\sigma\in\Sigma$, it is equivalent to require
        \[\varepsilon^{1_{i>n}}d_{\sigma(i)+n}d_{\sigma(i)}=\varepsilon^{1_{i>n}}d_{\sigma(i+n)}d_{\sigma(i)}=\varepsilon^{1_{\sigma(i)>n}}\]
        for each index $i$. Observe $\varepsilon^{1_{(i+n)>n}}=-\varepsilon^{1_{i>n}}$ and $\varepsilon^{1_{\sigma(i+n)>n}}=-\varepsilon^{1_{\sigma(i)>n}}$ (indices are still taken$\pmod{2n}$), so if the above equation is satisfied at index $i$, then it is satisfied at index $i+n$.
        
        As such, given signs $\{d_{\sigma(1)},\ldots,d_{\sigma(n)}\}$, we must set $d_{\sigma(i)+n}\coloneqq\varepsilon^{1_{\sigma(i)>n}}d_{\sigma(i)}$ for each $i\in\{1,2,\ldots,2\}$ to satisfy the equation at the indices $i\in\{1,2,\ldots,n\}$, and this choice of signs will work.
        \qedhere
    \end{enumerate}
\end{proof}
\begin{remark}
    Let's provide a convenient choice of signs $d_w$ for $w\in W$. If $G\in\{\GO_{2n},\O_{2n}\}$, then $\varepsilon=1$, so we see that $d_w\coloneqq1_{2n}$ will always work. If $G\in\{\GSp_{2n},\Sp_{2n}\}$, then we claim that we can put signs $d_w$ on the top-right quadrant of $\sigma_w$. Explicitly, we take $d_{\sigma(i)}=-1$ if $i\le n$ and $\sigma(i)>n$, and we take $d_{\sigma(i)}=1$ otherwise. The computation of the above lemma shows that this works (note $\varepsilon=-1$).
\end{remark}
Our benefit to having explicit representatives of $W$ is that we get explicit representatives of certain double quotients. For example, $W$ itself provides representatives of $B\backslash G/B$ by the Bruhat decomposition, where $B\subseteq G$ is a Borel subgroup containing $T$. We will be interested in $P\backslash G/P$.
\begin{lemma} \label{lem:compute-pgp}
    For each $r\in\{0,1,\ldots,n\}$, define
    \[\eta_r\coloneqq\begin{bmatrix}
        1_{n-r} \\ &&& \varepsilon1_r \\
        && 1_{n-r} \\
        & 1_r
    \end{bmatrix}.\]
    Then $\{\eta_0,\ldots,\eta_r\}\subseteq G$ provides a set of representatives of the double quotients $P\backslash G/P$.
\end{lemma}
\begin{proof}
    Even though the statement is rather uniform in $G$, the proof will require some moderate casework. Intuitively, what is going on here is that the Weyl group $W_P$ of $P$ consists of permutation matrices in the top-left and bottom-right quadrants, so to compute $P\backslash G/P\cong W_P\backslash W/W_P$, one can correct an arbitrary Weyl element $w\in W$ into the above form by row-reduction into the above form. We will argue slightly more directly.

    Before doing anything serious, we check that $\eta_r\in G$ in all cases. Observe that
    \[\eta_r^\intercal J\eta_r=\begin{bmatrix}
        1_{n-r} \\ &&& 1_r \\
        && 1_{n-r} \\
        & \varepsilon1_r
    \end{bmatrix}\begin{bmatrix}
        && \varepsilon1_{n-r} \\
        &&& \varepsilon1_r \\
        1_{n-r} \\
        & 1_r
    \end{bmatrix}\begin{bmatrix}
        1_{n-r} \\ &&& \varepsilon1_r \\
        && 1_{n-r} \\
        & 1_r
    \end{bmatrix}=J\]
    by a direct computation. So if $\varepsilon=1$, we see $\eta_r\in\O_{2n}$; and if $\varepsilon=-1$, we see $\eta_r\in\Sp_{2n}\subseteq\SL_{2n}$.

    It remains to show that $\{\eta_0,\ldots,\eta_n\}$ provides a set of representatives. We define a function $\rho\colon G\to\{0,\ldots,n\}$ by $\rho\left(\begin{bsmallmatrix}
        A & B \\ C & D
    \end{bsmallmatrix}\right)\coloneqq\op{rank}C$. Note that $\rho$ is surjective because $\rho(\eta_r)=r$ for each $r\in\{0,\ldots,n\}$. We will show that $\rho$ descends to a bijection $P\backslash G/P\to\{0,\ldots,n\}$, from which the result follows because $\rho(\eta_r)=r$.

    To begin, we show that $\rho$ descends to a surjection $P\backslash G/P\to\{0,\ldots,n\}$. Because $\rho$ is already a surjection, we just need to check that $\rho$ is defined up to double cosets. Well, we compute that
    \[\rho\left(\begin{bmatrix}
        A' & B' \\ & D'
    \end{bmatrix}\begin{bmatrix}
        A & B \\ C & D
    \end{bmatrix}\begin{bmatrix}
        A'' & B'' \\ & D''
    \end{bmatrix}\right)=\rho\left(\begin{bmatrix}
        * & * \\
        D'CA'' & *
    \end{bmatrix}\right)=\op{rank}D'CA'',\]
    where $*$ indicates some value we have not bothered to compute. Now, multiplication by an invertible matrix does not adjust rank, so $\op{rank}D'CA''=\op{rank}C=\rho\left(\begin{bsmallmatrix}
        A & B \\ C & D
    \end{bsmallmatrix}\right)$.

    It remains to show that the function $\rho\colon P\backslash G/P\to\{0,
    \ldots,n\}$ is injective. Unwinding definitions, it is enough to show that we must show that $\rho(g)=r$ implies that $g\in P\eta_rP$. Choosing a Borel subgroup $B\subseteq P$ containing $T$, we may use the Bruhat decomposition to see that $B\backslash G/B$ is represented by the Weyl group $W$. Thus, we may assume that $g=w=d_w\sigma_w$ where $d_w\in T$ and $\sigma_w$ is a permutation matrix. Now, to use that $\rho(d_w\sigma_w)=r$, we note that $\sigma_w$ being a permutation matrix means that the rank of the bottom-left quadrant is just
    \[r=\rho(d_w\sigma_w)=\#\{i\le n:\sigma(i)>n\}.\]
    Now, we may choose a permutation $\sigma$ of $\{1,\ldots,n\}$ so that
    \[\{i\in\{1,\ldots,n\}:\sigma_w\sigma(i)>n\}=\{1,\ldots,r\}.\]
    If $G\in\{\GL_{2n},\SL_{2n}\}$, then we extend $\sigma$ to $S_{2n}$ by requiring $\{i>n:\sigma_w\sigma(i)\le n\}=\{n+1,\ldots,n+r\}$. Otherwise, $\sigma$ may be extended to a permutation in $\Sigma$ (from \Cref{lem:weyl-normal-form}), and we can see that $\sigma\in D_{2n}^{\mathrm{sp}}$. Thus, $\sigma$ belongs to some Weyl element in $W\cap P$, so multiplication on the right of $w$ by this Weyl element, we may assume that
    \begin{align*}
        \{i\le n:\sigma_w(i)>n\} &= \{1,\ldots,r\} \\
        \{i>n:\sigma_w(i)>n\} &= \{n+1,\ldots,n+r\}
    \end{align*}
    on the nose. A similar argument by multiplying on the left of $w$ is able to rearrange the actual values of $\sigma_w$ to show that $w$ has the same underlying permutation matrix as $\eta_r$, so $w=\eta_r\in P\eta_rP$ follows.
\end{proof}
\begin{remark}
    A benefit of the above proof with $\rho$ (instead of arguing with $W_P\backslash W/W_P$) is that we see that the double cosets
    \[P\eta_rP=\left\{\begin{bmatrix}
        A & B \\ C & D
    \end{bmatrix}\in G:\op{rank}C=r\right\}\]
    are all (Zariski) locally closed. In fact, $P\eta_0P$ is (Zariski) closed, and $P\eta_rP$ is the only (Zariski) open double coset (defined by $\det C\ne0$).
\end{remark}

% maybe compute P\G/U for the spherical vector, but maybe I'll worry about that later

\subsection{Parabolic Induction}
In the sequel, we will be interested in the representations $\Ind_P^G\chi$ where $\chi\colon P\to\CC^\times$ is a character. We spend this subsection collecting a few facts about these representations. In particular, we will show that these representations are multiplicity-free and irreducible for ``general'' $\chi$.

Let's begin by showing our generic irreducibility.
\begin{proposition} \label{prop:ind-irred}
    Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Then
    \[\dim\End_G\Ind_P^G\chi=\begin{cases}
        n+1 & \text{if }\beta=1, \\
        2 & \text{if }\beta^2=1,\beta\ne1\text{ and }G=\SL_{2n}, \\
        n+1 & \text{if }\beta^2=1,\beta\ne1\text{ and }G\in\{\O_{2n},\Sp_{2n}\}, \\
        \floor{\frac12(n+1)} & \text{if }\beta^2=1,\beta\ne1\text{ and }G\in\{\GO_{2n},\GSp_{2n}\}, \\
        1 & \text{else}.
    \end{cases}\]
    In particular, $\Ind_P^G\chi$ is irreducible provided $\beta^2\ne1$.
\end{proposition}
\begin{proof}
    We use Mackey theory in the form of \cite[Theorem~32.1]{bump-lie-group}. Namely, we are interested in computing the dimension of the space
    \[\mc H\coloneqq\{f\in\op{Mor}(G,\CC):f(p_1gp_2)=\chi(p_1)\chi(p_2)f(g)\text{ for }p_1,p_2\in P,g\in G\}.\]
    Thus, we see that $f\in \mc H$ is uniquely determined by its values on representatives of the double cosets $P\backslash G/P$. As such, we set $f_r\in \mc H$ to be supported on $P\eta_rP$ defined by $f_r(\eta_r)\in\{0,1\}$, where we take $f_r(\eta_r)=1$ provided that this gives a well-defined function in $\mc H$. Then our computation of $P\backslash G/P$ in \Cref{lem:compute-pgp} tells us that $\{f_r:f_r\ne0\}$ is a basis of $\mc H$.

    Thus, we are left computing the number of $r$ such that $f_r\in \mc H$ is well-defined with $f_r(\eta_r)=1$. So fix some $r$, and we check if $f_r\in \mc H$ is well-defined with $f_r(\eta_r)=1$. Namely, if $p_1\eta_rp_2=p_1'\eta_rp_2'$ for $p_1,p_1',p_2,p_2'\in P$, we must check that $\chi(p_1)\chi(p_2)=\chi(p_1')\chi(p_2')$. Rearranging, it is enough to check that $p_1=\eta_rp_2\eta_r^{-1}$ implies that $\chi(p_1)=\chi(p_2)$. In other words, if $p\in P$ has $\eta_rp\eta_r^{-1}\in P$, we need $\chi(p)=\chi\left(\eta_rp\eta_r^{-1}\right)$. Well, we write
    \[p\coloneqq\begin{bmatrix}
        A_1 & A_2 & B_1 & B_2 \\
        A_3 & A_4 & B_3 & B_4 \\
            &     & D_1 & D_2 \\
            &     & D_3 & D_4
    \end{bmatrix}\]
    to have the same dimensions as $\eta_r$, and then we see that
    \begin{align*}
        \eta_rp\eta_r^{-1} &= \begin{bmatrix}
            1_{n-r} \\
            &&& \varepsilon 1_{r} \\
            && 1_{n-r} \\
            & 1_{r}
        \end{bmatrix}\begin{bmatrix}
            A_1 & A_2 & B_1 & B_2 \\
            A_3 & A_4 & B_3 & B_4 \\
                &     & D_1 & D_2 \\
                &     & D_3 & D_4
        \end{bmatrix}\begin{bmatrix}
            1_{n-r} \\
            &&& \varepsilon 1_{r} \\
            && 1_{n-r} \\
            & 1_{r}
        \end{bmatrix}^{-1} \\
        &= \begin{bmatrix}
            A_1 & \varepsilon B_2 &  B_1 & A_2 \\
                &  D_4 & \varepsilon D_3 \\
                & \varepsilon D_2 &  D_1 \\
            A_3 & \varepsilon B_4 &  B_3 & A_4
        \end{bmatrix},
    \end{align*}
    so this is in $P$ if and only if $A_3=B_4=D_2=0$. Thus, $\chi(p)=\chi\left(\eta_rp\eta_r^{-1}\right)$ is equivalent to always having
    \[\chi\left(\begin{bmatrix}
        A_1 & \varepsilon B_2 &  B_1 & A_2 \\
            &  D_4 & \varepsilon D_3 \\
            &      &  D_1 \\
            &      &  B_3 & A_4
    \end{bmatrix}\right)\stackrel?=\chi\left(\begin{bmatrix}
        A_1 & A_2 & B_1 & B_2 \\
            & A_4 & B_3 &     \\
            &     & D_1 &     \\
            &     & D_3 & D_4
    \end{bmatrix}\right).\]
    The multiplier of the left-hand side is $m\left(\eta_rp\eta_r^{-1}\right)=m(p)$, which is also the multiplier of the right-hand side, so we are allowed to ignore $\alpha$ for the rest of the proof. (The point is that $m$ is defined as a character on $G$.) As for $\beta$, we go ahead and compute $\chi_{\det}$ on both sides to see that we must have
    \[\beta(\det D_1\cdot\det A_4)^{-1}\stackrel?=\beta(\det D_1\cdot\det D_4)^{-1},\]
    where we take the convention that the ``empty'' matrix has determinant $1$. Equivalently, we are asking for
    \[\beta(\det A_4)\stackrel?=\beta(\det D_4).\]
    We now work in cases on $G$ and $r$.
    \begin{itemize}
        \item If $r=0$, then $A_4$ and $D_4$ are empty, so the condition holds. Thus, we will take $r>1$ in the rest of our casework.
        \item If $\beta=1$, then the condition holds. Thus, we will take $\beta\ne1$ in the rest of our casework.
        \item Take $G=\GL_{2n}$. Because $r>0$, $\det$ is always surjective, and here there are no conditions on how $\det A_4$ and $\det D_4$ should relate to each other, so the condition never holds.
        \item Take $G=\SL_{2n}$. Because $r>0$, $\det$ will always be surjective. If $r=n$, then the condition $\det p=1$ becomes $\det A_4=\det D_4^{-1}$, so we get a contribution in this case only when $\beta^2=1$. Otherwise, $r\notin\{0,n\}$, so $\det A_4$ and $\det D_4$ can be arbitrary elements of $\FF_q^\times$ (our condition $\det p=1$ only requires $\det A_1D_4D_1A_4=1$), so the condition never holds.
        \item Take $G\in\{\O_{2n},\Sp_{2n}\}$. Then $A_4=D_4^{-\intercal}$, so we are requiring $\beta(\det A_4)^2=1$. Because $\det$ is surjective when $r>0$, nonzero $r$ contribute in this case exactly when $\beta^2=1$.
        \item Take $G\in\{\GO_{2n},\GSp_{2n}\}$. Then $A_4=m(p)D_4^{-\intercal}$, so we are requiring
        \[\beta(\det A_4)^2=\beta(m(p))^r.\]
        With $r>0$, the values $\det A_4$ and $m(p)$ are arbitrary elements of $\FF_q^\times$, so we would like for $\beta(x)^2=\beta(y)^r$ for any $x,y\in\FF_q^\times$. Taking $y=1$ shows that we will only get contributions in this case when $\beta^2=1$, and taking $x=1$ shows that we will only get contributions when $\beta^r=1$ too. However, with $\beta\ne1$, we see that $\beta^r=1$ only happens when $r$ is even.
    \end{itemize}
    Tallying the above cases completes the proof.
\end{proof}
\begin{remark}
    In the sequel, we will make frequent use of the basis of $f_\bullet$s of $\mc H$.
\end{remark}
Even though it is not currently relevant to our discussion, we will want a similar Mackey theory computation in the future, so we will get it out of the way now. We require a definition.
\begin{definition} \label{def:chi-j}
    Note that the element $J$ normalizes $M$: for any $\begin{bsmallmatrix}
		A \\ & D
	\end{bsmallmatrix}\in M$, we see that $J^{-1}\begin{bsmallmatrix}
		A \\ & D
	\end{bsmallmatrix}J=\begin{bsmallmatrix}
		D \\ & A
	\end{bsmallmatrix}\in M$. Thus, for any character $\chi\colon P\to\CC^\times$, we define the character $\chi^J$ as the following composite.
	\[\arraycolsep=1.4pt\begin{array}{cccccccc}
		P &\onto& M &\stackrel J\cong& M &\stackrel\chi\to& \CC^\times \\
		\begin{bsmallmatrix}
			A & B \\ & D
		\end{bsmallmatrix} &\mapsto& \begin{bsmallmatrix}
			A \\ & D
		\end{bsmallmatrix} &\mapsto& \begin{bsmallmatrix}
			D \\ & A 
		\end{bsmallmatrix} &\mapsto& \chi\left(\begin{bsmallmatrix}
			D \\ & A 
		\end{bsmallmatrix}\right)
	\end{array}\]
\end{definition}
\begin{remark}
	The importance of this definition arises in \Cref{lem:parabolic-intertwine}.
\end{remark}
\begin{remark}
	Write $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Then we will evaluate $\chi^J$ on $p\coloneqq\begin{bsmallmatrix}
		A & B \\ & D
	\end{bsmallmatrix}$ as
	\[\chi^J\left(\begin{bmatrix}
		A & B \\ & D
	\end{bmatrix}\right)=\chi\left(\begin{bmatrix}
		D \\ & A
	\end{bmatrix}\right).\]
	If $G\in\{\GL_{2n},\SL_{2n}\}$, then this is $\alpha(\det AD)\beta(\det A)^{-1}=(\alpha/\beta)(\det AD)\beta(\det D)$, so we see that $\chi^J=\left(\alpha\beta^{-1}\circ m\right)\left(\beta^{-1}\circ\chi_{\det}\right)$. Otherwise, if $G\in\{\GO_{2n},\O_{2n},\GSp_{2n},\Sp_{2n}\}$, then we rewrite $p$ as $\begin{bsmallmatrix}
		\lambda A & B \\ & A^{-\intercal}
	\end{bsmallmatrix}$ so that we get $\chi^J(p)=\alpha(\lambda)\beta(\det\lambda A)^{-1}$, so $\chi^J=\left(\alpha\beta^{-n}\circ m\right)\left(\beta^{-1}\circ\chi_{\det}\right)$.
	
	In particular, all cases find that $\left(\chi^J\right)^J=\chi$. Further, if $\beta=1$, then $\chi^J=\chi$; alternatively, if we only have $\beta^2=1$ but $G\in\{\SL_{2n},\O_{2n},\Sp_{2n}\}$ so that $m=1$, then we still have $\chi^J=\chi$.
\end{remark}
\begin{proposition} \label{prop:twisted-ind-basis}
	Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Then we compute a basis for $\left(\Ind_P^G\chi\right)^{\chi^J}$. In particular, we find
	\[\dim\left(\Ind_P^G\chi\right)^{\chi^J}=\dim\left(\Ind_P^G\chi\right)^{\chi}.\]
\end{proposition}
\begin{proof}
	We proceed as in \Cref{prop:ind-irred}. For brevity, set $\mc H_J\coloneqq\left(\Ind_P^G\chi\right)^{\chi^J}$. Again, $f\in\mc H_J$ is uniquely determined by its values on representatives of $P\backslash G/P$, so we set $f_r\in\mc H_J$ to be supported on $P\eta_rP$ defined by $f_r(\eta_r)\in\{0,1\}$ where we take $f_r(\eta_r)=1$ whenever possible; thus, $\{f_r:f_r\ne0\}$ is a basis of $\mc H_J$.

	Continuing, as in \Cref{prop:ind-irred}, we are checking which $f_r\in\mc H_J$ are well-defined with $f_r(\eta_r)=1$. Namely, for $p_1,p_2,p_1',p_2'\in P$, we must have $\chi(p_1)\chi^J(p_2)=f_r(p_1\eta_rp_2)=f_r(p_1'\eta_rp_2')=\chi(p_1')\chi^J(p_2)$. Rearranging, it is enough to check that $p_1=\eta_rp_2\eta_r^{-1}$ implies that $\chi(p_1)=\chi^J(p_2)$. In other words, if $p\in P$ has $\eta_rp\eta_r^{-1}\in P$, we need $\chi(p)=\chi\left(\eta_rp\eta_r^{-1}\right)$. Writing
	\[p\coloneqq\begin{bmatrix}
        A_1 & A_2 & B_1 & B_2 \\
        A_3 & A_4 & B_3 & B_4 \\
            &     & D_1 & D_2 \\
            &     & D_3 & D_4
    \end{bmatrix}\]
    to have the same dimensions as $\eta_r$, and then we see that
    \[\eta_rp\eta_r^{-1} = \begin{bmatrix}
		A_1 & \varepsilon B_2 &  B_1 & A_2 \\
			&  D_4 & \varepsilon D_3 \\
			& \varepsilon D_2 &  D_1 \\
		A_3 & \varepsilon B_4 &  B_3 & A_4
	\end{bmatrix},\]
	so this is in $P$ if and only if $A_3=B_4=D_2=0$. Thus, $\chi(p)=\chi^J\left(\eta_rp\eta_r^{-1}\right)$ is equivalent to always having
	\[\chi\left(\begin{bmatrix}
        A_1 & A_2 & B_1 & B_2 \\
            & A_4 & B_3 &     \\
            &     & D_1 &     \\
            &     & D_3 & D_4
    \end{bmatrix}\right)\stackrel?=\chi^J\left(\begin{bmatrix}
        A_1 & \varepsilon B_2 &  B_1 & A_2 \\
            &  D_4 & \varepsilon D_3 \\
            &      &  D_1 \\
            &      &  B_3 & A_4
    \end{bmatrix}\right).\]
	We now do casework on $G$ and $r$.
	\begin{itemize}
		\item If $G\in\{\O_{2n},\Sp_{2n}\}$, then $\chi^J=\beta^{-1}\circ\chi_{\det}$, so we are asking for
		\[\beta(\det D_1\det D_4)^{-1}\stackrel?=\beta(\det D_1\det A_4).\]
		In this case, $A_4=D_4^{-\intercal}$, so we see that the above is equivalent to $\beta(\det D_1)^2=1$. If $r=n$, then we always get a contribution because $D_1$ is empty; otherwise, $\det$ is surjective, so we get contributions only when $\beta^2=1$.
		\item If $G=\SL_{2n}$, then $\chi^J=\beta^{-1}\circ\chi_{\det}$, so we are still asking for
		\[\beta(\det D_1\det D_4)^{-1}\stackrel?=\beta(\det D_1\det A_4).\]
		This simplifies to
		\[\beta\left(\det A_4\det D_1^2\det D_4\right)=\beta(\det A_1)^{-1}\beta(\det D_1)\stackrel?=1.\]
		Now, if $r=n$, then $A_1$ and $D_1$ is empty, so we are asking for $\beta(\det A_4D_4)=1$, which is true because $\det A_4D_4=1$. If $r=0$, then we must have $\det A_1D_1=1$, so we get a contribution provided $\beta^2=1$. Otherwise, with $r\notin\{0,n\}$, the determinants $\det A_1$ and $\det D_1$ are arbitrary, so we only get a contribution when $\beta=1$.
		\item If $G=\GL_{2n}$, then $\chi^J=\left(\alpha\beta^{-1}\circ m\right)\left(\beta^{-1}\circ\chi_{\det}\right)$, so we are asking for
		\[\alpha\left(\det A_1D_1\det A_4D_4\right)\beta(\det D_1\det D_4)^{-1}=\alpha\beta^{-1}\left(\det A_1D_1\det A_4D_4\right)\beta(\det D_1\det A_4).\]
		Here, we see that $\alpha$ cancels on both sides, so we may ignore it. Rearranging, this is equivalent to
		\[\beta(\det D_1)=\beta\left(\det A_1\right).\]
		If $r=n$, then $D_1$ and $A_4$ are empty, so this condition holds. Otherwise, with $r<n$, these determinants are basically arbitrary, so we only get a contribution when $\beta=1$.
		\item If $G\in\{\GO_{2n},\GSp_{2n}\}$, then $\chi^J=\left(\alpha\beta^{-n}\circ m\right)\left(\beta^{-1}\circ\chi_{\det}\right)$. Additionally, letting $\lambda\in\FF_q^\times$ be the multiplier, we see that $A_4=\lambda D_4^\intercal$, so we are asking for
		\[\alpha(\lambda)\beta(\det D_1\det D_4)^{-1}=\alpha\beta^{-n}(\lambda)\beta\left(\det D_1\det \lambda D_4^{-\intercal}\right).\]
		Once again, $\alpha$ cancels on both sides, so we ignore it. Now, this rearranges to
		\[\beta(\det D_1)^{-2}=\beta^{r-n}(\lambda).\]
		As usual, if $r=n$, then the right-hand side is $1$, and the left-hand side is $1$ because $D_1$ is empty, so we will get a contribution. Otherwise, $\det$ is surjective, so we will get a contribution only when $\beta^2=1$ and $\beta^{r-n}=1$. So $\beta=1$ is always okay, and the case where $\beta^2=1$ while $\beta\ne1$ only takes $f_r$ where $r\equiv n\pmod2$.
	\end{itemize}
	Tallying the above cases and comparing with \Cref{prop:ind-irred} completes the proof.
\end{proof}

We now show that $\Ind_P^G\chi$ is multiplicity-free.
\begin{proposition} \label{prop:ind-mult-free}
    For any character $\chi\colon P\to\CC^\times$, the representation $\Ind_P^G\chi$ is multiplicity-free.
\end{proposition}
\begin{proof}
    Write $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$, as usual. If $\beta^2\ne1$, then \Cref{prop:ind-irred} tells us that $\Ind_P^G\chi$ is irreducible. It remains to handle the case where $\beta^2=1$. By \cite[Theorem~45.1]{bump-lie-group}, it suffices for the Hecke algebra $\mc H$ of functions
    \[\mc H\coloneqq\{f\in\op{Mor}(G,\CC):f(p_1gp_2)=\chi(p_1)\chi(p_2)f(g)\text{ for }p_1,p_2\in P,g\in G\},\]
    with product given by the convolution, to be commutative. We will split this into two cases.
    \begin{itemize}
        \item Take $G=\SL_{2n}$ where $\chi\ne1$. We will apply force. Here, $\alpha=1$, so we still have $\chi^2=1$. Then the computation of \Cref{prop:ind-irred} tells us that $\mc H$ has $\CC$-basis by the functions $f_0,f_n\colon G\to\CC$ where $f_r$ is supported on $P\eta_rP$ with $f_r(\eta_r)=1$. Thus, to check that $\mc H$ is commutative, it is enough to verify that $f_0*f_n=f_n*f_0$. We will do this by explicit computation. Note that $f\in\mc H$ satisfies $f=f(\eta_0)f_0+f(\eta_n)f_n$, so it is enough to check that
        \[(f_0*f_n)(\eta_r)\stackrel?=(f_n*f_0)(\eta_r)\]
        for $r\in\{0,n\}$. We do this by explicit computation. For $\eta_0=1_{2n}$, we note
        \[(f_0*f_n)(\eta_0)=\sum_{h\in P\backslash G}f_0\left(h^{-1}\right)f_n(h).\]
        Now, $f_0$ and $f_n$ have disjoint supports, so this sums to $0$. A symmetric argument shows $(f_n*f_0)(\eta_0)=0$.
        
        On the other hand, we see
        \[(f_0*f_n)(\eta_n)=\sum_{h\in P\backslash G}f_0\left(\eta_nh^{-1}\right)f_n(h)=\sum_{h\in P\backslash G}f_0\left(h^{-1}\right)f_n(h\eta_n).\]
        Now, $f_0$ is supported on $P$, so the only nonzero term of the sum is at the identity coset of $P\backslash G$, so this evaluates to $f_n(\eta_n)$. A similar argument shows that
        \[(f_n*f_0)(\eta_n)=\sum_{h\in P\backslash G}f_n\left(\eta_nh^{-1}\right)f_0(h)\]
        equals $f_n(\eta_n)$ again, completing the proof.
        
        \item Take $G\in\{\SL_{2n},\O_{2n},\Sp_{2n}\}$, except the above case. Again, $\alpha=1$, so $\chi^2=1$; namely, $\chi=\chi^{-1}$. We will apply an argument similar to \cite[Theorem~45.2]{bump-lie-group}. In particular, define $\iota\colon G\to G$ by $\iota(g)\coloneqq g^{-1}$. For example, we see that $\iota(\iota(g))=g$ and $\iota(gh)=\iota(h)\iota(g)$ for $g\in G$ and $\chi(\iota(p))=\chi(p)^{-1}=\chi(p)$ for $p\in P$, which can be checked directly. Thus, we may define an operator $(\cdot)^\iota\colon\mc H\to\mc H$ by
        \[f^\iota(g)\coloneqq f(\iota(g)).\]
        To see that $f^\iota\in\mc H$, we simply must check that
        \[f^\iota(p_1gp_2)=f\left(p_2^{-1}g^{-1}p_1^{-1}\right)=\chi(\iota(p_2))\chi(\iota(p_1))f(\iota(g))=\chi(p_1)\chi(p_2)f^\iota(g).\]
        Now, $(\cdot)^\iota$ is of course $\CC$-linear, and we claim that it is anti-commutative: for $f,f'\in\mc H$, we compute
        \[(f*f')^\iota(g)=\sum_{h\in P\backslash G}f\left(g^{-1}h^{-1}\right)f'(h)=\sum_{h\in P\backslash G}f\left(hg^{-1}\right)f'\left(h^{-1}\right)=(f'^\iota*f^\iota)(g).\]
        However, we claim that $(\cdot)^\iota$ is in fact the identity map on $\mc H$, from which it follows that $\mc H$ is commutative. Well, fix some $f\in\mc H$, and we must show that $f^\iota=f$. By \Cref{lem:compute-pgp}, we see that $f$ is uniquely determined by its values on the $\eta_r$ for $r\in\{0,\ldots,n\}$, so it is enough to check that $f\left(\eta_r^{-1}\right)=f(\eta_r)$. Well,
        \[\eta_r^{-1} = \begin{bmatrix}
            1_{n-r} \\
            &&& 1_{r} \\
            && 1_{n-r} \\
            & \varepsilon 1_{r}
        \end{bmatrix} = \begin{bmatrix}
            1_{n-r} \\ & \varepsilon1_{r} \\ && 1_{n-r} \\ &&& \varepsilon1_{r}
        \end{bmatrix}\begin{bmatrix}
            1_{n-r} \\
            &&& \varepsilon 1_{r} \\
            && 1_{n-r} \\
            & 1_{r}
        \end{bmatrix},\]
        so
        \[f\left(\eta_r^{-1}\right)=\chi\left(\begin{bmatrix}
            1_{n-r} \\ & \varepsilon1_{r} \\ && 1_{n-r} \\ &&& \varepsilon1_{r}
        \end{bmatrix}\right)f(\eta_r).\]
        If $\chi=1$, then the extra factor here goes away, so we are safe. Otherwise, $\chi\ne1$. Continuing, if $G=\O_{2n}$, then $\varepsilon=1$, so the extra factor here still goes away. Otherwise, $\varepsilon=-1$. If $G=\Sp_{2n}$ with $\chi\ne1$, then the proof of \Cref{prop:ind-irred} tells us that we only have to pay attention to the case where $r$ is even, and here the Siegel determinant of the matrix in question is $1$, so we are okay. Lastly, we have dealt with the case where $G=\SL_{2n}$ and $\chi\ne1$ in the previous point.

        \item Take $G\in\{\GL_{2n},\GO_{2n},\GSp_{2n}\}$. Let $S\coloneqq\ker m$ so that $S\in\{\SL_{2n},\O_{2n},\Sp_{2n}\}$. Now, let $\mc H^S$ denote the Hecke algebra corresponding to the group $S$ and character $\chi^S$ (which is the restriction of $\chi$), and we will set $\mc H^G\coloneqq\mc H$ and $\chi^G\coloneqq\chi$. (In short, $\chi^S$ basically ignores $\alpha$.) We will show that $\mc H^S$ surjects onto $\mc H^G$, which shows that $\mc H_G$ is commutative.\footnote{The argument of the previous point does not directly apply to this case because it requires $\chi^2=1$, which is not true when $\alpha$ is allowed to be general.}
        
        For each $r\in\{0,\ldots,n\}$, let $f_r^G\in\mc H^G$ and $f_r^S\in\mc H^S$ denote the functions on the corresponding group supported on the double coset of $\eta_r$ with $f_r^\bullet(\eta_r)=1$ whenever possible. Then the set of nonzero $f_r^\bullet$ forms a basis of $\mc H^\bullet$ as discussed in the proof of \Cref{prop:ind-irred}. In fact, a careful reading of the computation in \Cref{prop:ind-irred} shows that $f_r^G\ne0$ implies that $f_r^S\ne0$ for each $r$, so we may construct a $\CC$-linear surjection $\pi\colon\mc H^S\to\mc H^G$ by sending $\pi\colon f_r^S\mapsto f_r^G$ (for $f_r^S\ne0$, which is our basis).

        It remains to show that this map is multiplicative. Fix indices $r,s,t\in\{0,\ldots,n\}$ with $f_r^G,f_s^G,f_t^G\ne0$, and we claim that
        \[\left(f_r^G*f_s^G\right)(\eta_t)\stackrel?=\left(f_r^S*f_s^S\right)(\eta_t).\]
        To see why this is enough, we let $t$ vary to note that this would imply
        \begin{align*}
            f_r^G*f_s^G &= \sum_{t=0}^n\left(f_r^G*f_s^G\right)(\eta_t)f_t^G \\
            &= \sum_{t=0}^n\left(f_r^S*f_s^S\right)(\eta_t)\pi\left(f_t^S\right) \\
            &= \pi\left(\sum_{t=0}^n\left(f_r^S*f_s^S\right)(\eta_t)f_t^S\right) \\
            &= \pi\left(f_r^S*f_s^S\right).
        \end{align*}
        It remains to show the claim. Expanding out the convolution, we are being asked to show that
        \[\sum_{h\in P^G\backslash G}f_r^G\left(\eta_th^{-1}\right)f_s^G(h)\stackrel?=\sum_{h\in P^S\backslash S}f_r^S\left(\eta_th^{-1}\right)f_s^S(h),\]
        where $P^G\subseteq G$ and $P^S\subseteq S$ are the Siegel parabolic subgroups. We will show that these two sums are equal term-wise.

        For the claim that the two sums are equal term-wise, we quickly verify that the inclusion $S\subseteq G$ induces a bijection $P^S\backslash S\to P^G\backslash G$. Well, this map is certainly well-defined: if $s,s'\in S$ are in the same class$\pmod{P^S}$, they will be in the same class$\pmod{P^G}$. Continuing, this map is injective because having $s=ps'$ for $s,s'\in S$ and $p\in P^G$ implies $p\in P^G\cap S$, so $s$ and $s'$ are in the same class in $P^S\backslash S$. Lastly, to see that this map is surjective, we must show that any $g\in G$ can be written as $ps$ where $p\in P^G$ and $s\in S$. Equivalently, we want to show that the composite $P^G\subseteq G\onto G/S$ is surjective, but $m\colon G/S\to\FF_q^\times$ is an isomorphism, so we want to show that $m\colon P^G\to\FF_q^\times$ is surjective. This can be seen by explicit example in all cases of $G$.

        We now show that our sums are equal ``term-wise.'' In light of the previous paragraph, it is enough to show that
        \[f_r^G\left(\eta_th^{-1}\right)f_s^G(h)\stackrel?=f_r^S\left(\eta_th^{-1}\right)f_s^S(h)\]
        for any $h\in S$. We quickly claim that $f_s^G(h)\ne0$ if and only if $f_s^S(h)\ne0$, and an analogous argument is able to show that $f_r^G\left(\eta_th^{-1}\right)\ne0$ if and only if $f_r^S\left(\eta_th^{-1}\right)$. Looking at the support of $f_s^G$ and $f_s^S$, we see that we are basically trying to show $S\cap P^G\eta_sP^G=P^S\eta_sP^S$. Well, for concreteness, write $h\coloneqq\begin{bsmallmatrix}
            A & B \\ C & D
        \end{bsmallmatrix}$. Then $h\in P^G\eta_sP^G$ if and only if $\op{rank}C=s$ by the proof of \Cref{lem:compute-pgp}, which is equivalent to $h\in P^S\eta_sP^S$, as required.

        In light of the previous paragraph, we now may assume that $f_r^S\left(\eta_th^{-1}\right)f_s^S(h)\ne0$. Then $h=p_1\eta_sp_2$ and $\eta_th^{-1}=p_1'\eta_sp_2'$ for $p_1,p_2,p_1',p_2'\in P_S$. Then we see
        \[f_r^G\left(\eta_th^{-1}\right)f_s^G(h)=\chi^G(p_1'p_2'p_1p_2)=\chi^S(p_1'p_2'p_1p_2)=f_r^S\left(\eta_th^{-1}\right)f_s^S(h),\]
        as desired.
        \qedhere
    \end{itemize}
\end{proof}
In the sequel, we will be interested in $G$-invariant operators on $\Ind^G_P\chi$, so it will be worth our time to provide a basis of sorts for this space. The main idea is as follows.
\begin{lemma} \label{lem:basis-of-ind}
	Fix a character $\chi\colon P\to\CC^\times$. For each irreducible subrepresentation $\pi$ of $\Ind^G_P\chi$, there exists exactly one dimension of $\chi$-eigenvectors in $\pi$.
\end{lemma}
\begin{proof}
	We are being asked to show that $\dim\Hom_P\left(\chi,\Res^G_P\pi\right)=1$. By Frobenius reciprocity, this is just
	\[\dim\Hom_P\left(\chi,\Res^G_P\pi\right)=\dim\Hom_G\left(\pi,\Ind_P^G\chi\right).\]
	Because $\pi$ is an irreducible subrepresentation, this dimension is at least $1$, but $\Ind_P^G\chi$ is multiplicity-free by \Cref{prop:ind-mult-free}, so this dimension is at most $1$. This completes the proof.
\end{proof}
Thus, we note that we can understand operators on $\Ind^G_P\chi$ by merely understanding where they send a vector from each irreducible subrepresentation. Each irreducible subrepresentation contributes a basis element to $\left(\Ind^G_P\chi\right)^\chi$, so we may just understand how the operator behaves on $\left(\Ind^G_P\chi\right)^\chi$. Now, $\left(\Ind^G_P\chi\right)^\chi$ is exactly the underlying vector space of the corresponding Hecke algebra $\mc H$, so the computation of \Cref{prop:ind-irred} provides a basis for this space. Explicitly, we are told the set of $r\in\{0,\ldots,n\}$ such that we can define a nonzero basis vector $f_r\in\mc H$ supported on the double coset $P\eta_rP$ defined by $f_r(\eta_r)\coloneqq1$.

\subsection{The Intertwining Operator}
We are now ready to introduce the main character of our story, which is an operator $I$ on the space $\op{Mor}(G,\CC)=\Ind^G1$ defined by
\[(If)(g)\coloneqq\sum_{u\in U}f\left(J^{-1}ug\right).\]
Note that $I\colon\Ind^G1\to\Ind^G1$ is $G$-invariant. In more typical notation, $I$ is the intertwining operator $M_J$, where we view $J$ as representing a Weyl group element. The space $\Ind^G1$ is a little too big to be useful, so we will work the spaces $\Ind_P^G\chi$ instead, where $\chi\colon P\to\CC^\times$ is some character. As such, we should explain that $I$ behaves nicely on these spaces.
\begin{lemma} \label{lem:parabolic-intertwine}
    Fix a character $\chi\colon P\to\CC^\times$. Then $I$ restricts to a $G$-invariant map $\Ind_P^G\chi\to\Ind_P^G\chi^J$.
\end{lemma}
\begin{proof}
    We already know that $I$ is $G$-invariant, so the main point is to check that $If\in\Ind_P^G\chi^J$. Namely, for any $p\in P$ and $g\in G$, we must show that $If(pg)=\chi^J(p)If(g)$. We may decompose $p$ as $p=d_pu_p$ where $u_p\in U$ and $d_p\in M$. Because the sum in $If$ is $U$-invariant, we see that $If(d_pu_pg)=If(d_pg)$, and we know $\chi^J(u_p)=1$ by its construction. Thus, we may safely ignore $u_p$. As for $d_p$, we write
    \[If(d_pg)=\sum_{u\in U}f\left(J^{-1}d_pJ\cdot J^{-1}d_p^{-1}ud_pg\right)=\sum_{u\in U}\chi\left(J^{-1}d_pJ\right)f(J^{-1}ug)=\chi^J(d_p)If(g),\]
    as required.
\end{proof}
Our end goal is to understand the linear transformation $I\colon\Ind^G_P\chi\to\Ind^G_P\chi^J$. Some aspects of this operator are not so hard to see: $I$ is invertible because of its description as the (restriction of the) intertwining operator $M_J\colon\Ind^G_B\chi\to\Ind^G_B\chi^J$ (where $B\subseteq P$ is a suitable Borel subgroup containing $T$), which is known to be invertible.

Another way we would like to understand $I$ is to expand $I$ out as a matrix using the bases of \Cref{lem:basis-of-ind}, which we see makes $I$ into a linear transformation
\[\left(\Ind^G_P\chi\right)^\chi\to\left(\Ind^G_P\chi^J\right)^\chi,\]
both of which have explicit bases by the computations of \Cref{prop:ind-irred,prop:twisted-ind-basis}. Lastly, we will want to understand $I$ is through its eigenvalues. Notably, $\Ind_P^G\chi$ is multiplicity-free by \Cref{prop:ind-mult-free}, so $I$ must be diagonalizable; we will be able to see this explicitly. In general, $\chi\ne\chi^J$ while $\left(\chi^J\right)^J=\chi$, so only $I\circ I$ will be an operator (on $\Ind^G_P\chi$) that could possibly have eigenvalues. Thus, to compose matrix representations, we will also want to expand $I$ as a linear transformation
\[\left(\Ind^G_P\chi^J\right)^\chi\to\left(\Ind^G_P\chi\right)^\chi,\]
where we again have explicit bases.

In this section, we will not build enough tools to precisely describe the eigenvalues of $I$, but we will be able to provide the matrix representations. We begin with the easier generic case.
\begin{proposition} \label{prop:generic-intertwining}
	Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Suppose $\beta^2\ne1$. Then let $\{f_0\}$ be the basis of $\left(\Ind_P^G\chi\right)^\chi$ described in \Cref{prop:ind-irred}, and let $\left\{f_n^J\right\}$ be the basis of $\left(\Ind_P^G\chi^J\right)^\chi$ described in \Cref{prop:twisted-ind-basis}. Then
	\[If_0=f_n^J\qquad\text{and}\qquad If_n^J=\beta(\varepsilon)^n\left|U\right|f_0.\]
	In particular, $I\circ I$ is the scalar $\beta(\varepsilon)^n\left|U\right|$.
\end{proposition}
\begin{proof}
	Certainly $If_0\in\op{span}\left\{f_n^J\right\}$ and $If_n^J\in\op{span}\{f_0\}$ because we have given bases. Anyway, we do our computations separately.
	
	For $If_0$, we see that $If_0=If_0(\eta_n)f_n$, so we want to compute
	\[If_0(\eta_n)=\sum_{u\in U}f_0\left(J^{-1}u\eta_n\right).\]
	To compute the sum, we see $\eta_n=J=\begin{bsmallmatrix}
		& \varepsilon1_n \\ 1_n
	\end{bsmallmatrix}$, so we write $u\coloneqq\begin{bsmallmatrix}
		1 & B \\ & 1
	\end{bsmallmatrix}$ so that $J^{-1}uJ=\begin{bsmallmatrix}
		1 \\ \varepsilon B & 1
	\end{bsmallmatrix}$. Now, $f_0$ is nonzero $J^{-1}uJ$ only when $J^{-1}uJ\in P$, which we see only happens when $B=0$ so that $u=1_{2n}$ and $J^{-1}uJ=1_{2n}$. Thus, $If_0(\eta_n)=1$, as required.
	
	For $If_n^J$, we see that $If_n^J=If_n^J(\eta_0)f_0$, so we want to compute
	\[If_n^J(\eta_0)=\sum_{u\in U}f_n^J\left(J^{-1}u\eta_0\right)=\sum_{u\in U}\chi(u)f_n^J\left(J^{-1}\right)=\left|U\right|f_n^J\left(J^{-1}\right).\]
	Now, $J^{-1}=\begin{bsmallmatrix}
		& 1_n \\ \varepsilon1_n
	\end{bsmallmatrix}=\varepsilon J$, so $f_n^J\left(J^{-1}\right)=\chi(\varepsilon1_{2n})$. Plugging in for $\chi$ completes the proof.
\end{proof}
% \begin{remark}
% 	More generally, the above has shown that we always have $If_0(\eta_n)=1$ and $If_n(\eta_0)=\left|U\right|$, which will be reused and generalized in the harder computations below.
% \end{remark}

We now turn towards the case $\beta^2=1$. We begin by stating a general result which will help us with our subsequent computations.
\begin{lemma} \label{lem:matrix-coeff}
	Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Given $r,s\in\{0,\ldots,n\}$ where the usual basis vector $f_r\in\left(\Ind_P^G\chi\right)^\chi$ (of \Cref{prop:ind-irred}) is nonzero, we compute\todo{Check sign}
	\[If_r(\eta_s)=\beta(\varepsilon)^{n-s}Q\sum_{\substack{D\in\FF_q^{s\times s}\\\begin{bsmallmatrix}
		1_n & \op{diag}(D,0_{n-s}) \\ & 1_n
	\end{bsmallmatrix}\in G\\\op{rank}D=r+s-n}}\beta(\det E)^{-1},\]
	where
	\[Q\coloneqq\begin{cases}
		q^{n^2-s^2} & \text{if }G\in\{\GL_{2n},\SL_{2n}\}, \\
		q^{\binom{n}2-\binom{s}2} & \text{if }G\in\{\GO_{2n},\O_{2n}\}, \\
		q^{\binom{n+1}2-\binom{s+1}2} & \text{if }G\in\{\GSp_{2n},\Sp_{2n}\},
	\end{cases}\]
	and $E\in\GL_{r+s-n}(\FF_q)$ is some matrix defined from $D$ determined as follows: we always have $\begin{bsmallmatrix}
		E \\ & 0
	\end{bsmallmatrix}=D_1DD_2$ for $D_1,D_2\in\SL_s(\FF_q)$. Then if $G\in\{\GO_{2n},\O_{2n},\GSp_{2n},\Sp_{2n}\}$, we require $D_2=D_1^\intercal$. Lastly, if $G\in\{\GO_{2n},\O_{2n}\}$, we require $E=\begin{bsmallmatrix}
		& -1_{(r+s-n)/2} \\ 1_{(r+s-n)/2}
	\end{bsmallmatrix}$; and if $G\in\{\GSp_{2n},\Sp_{2n}\}$, we require $E$ to be diagonal.
\end{lemma}
\begin{proof}
	We are asked to compute $If_r(\eta_s)=\sum_{u\in U}f_r\left(J^{-1}u\eta_s\right)$. For this, we want to compute $J^{-1}u\eta_s$ and in particular want to ask when it lives in $P\eta_rP$. As such, we write $u$ in a block matrix form and compute
	\begin{align*}
		J^{-1}u\eta_s &= \begin{bmatrix}
			&& 1_{n-s} \\ &&& 1_s \\
			\varepsilon1_{n-s} \\ & \varepsilon1_s
		\end{bmatrix}\begin{bmatrix}
			1_{n-s} && A & B \\ & 1_s & C & D \\
			&& 1_{n-s} \\ &&& 1_s
		\end{bmatrix}\begin{bmatrix}
			1_{n-s} \\ &&& \varepsilon1_s \\
			&& 1_{n-s} \\ & 1_s
		\end{bmatrix} \\
		&= \varepsilon\begin{bmatrix}
			&& \varepsilon1_{n-s} \\ & \varepsilon1_s \\
			1_{n-s} & B & A \\ & D & C & \varepsilon1_s
		\end{bmatrix} \\
		&= \varepsilon\begin{bmatrix}
			&& \varepsilon1_{n-s} \\ & \varepsilon1_s \\
			1_{n-s} &   &   \\ & D &   & \varepsilon1_s
		\end{bmatrix}\begin{bmatrix}
			1_{n-s} & B & A \\ & 1_s \\
			&& 1_{n-s} \\ && \varepsilon C & 1_s
		\end{bmatrix}.
	\end{align*}
	Now, $\chi$ vanishes on the last rightmost matrix (it has Siegel determinant and multiplier both equal to $1$, so \Cref{lem:decompose-character} finishes), so to compute $f_r\left(J^{-1}u\eta_s\right)$, we see that $A,B,C$ in $u$ do not matter, so
	\[If_r(\eta_s)=Q\sum_{D\in\FF_q^{s\times s}}f_r\left(\begin{bmatrix}
		&& 1_{n-s} \\ & 1_s \\
		\varepsilon1_{n-s} &   &   \\ & \varepsilon D &   & 1_s
	\end{bmatrix}\right).\]
	(Here, $Q$ counts the number of ways to choose $A,B,C$.) Of course, we may replace $D$ with $\varepsilon D$, so in fact this sum is
	\[If_r(\eta_s)=Q\sum_{D\in\FF_q^{s\times s}}f_r\left(\begin{bmatrix}
		&& 1_{n-s} \\ & 1_s \\
		\varepsilon1_{n-s} &   &   \\ & D &   & 1_s
	\end{bmatrix}\right).\]
	Now, $f_r$ is supported on $P\eta_rP$, so by \Cref{lem:compute-pgp}, we see that $D$ gives a nonzero contribution if and only if $\op{rank}\begin{bsmallmatrix}
		1_{n-s} \\ & D
	\end{bsmallmatrix}=r$, which is equivalent to requiring $\op{rank}D=r+s-n$, which we will assume from now on. Set $d\coloneqq\op{rank}D$ for brevity.
	
	We now place $D$ into a normal form. This requires some casework on $G$.
	\begin{itemize}
		\item If $G\in\{\GL_{2n},\SL_{2n}\}$, then we use row-reduction to find matrices $D_1,D_2\in\SL_{s}(\FF_q)$ such that $D_1DD_2$ takes the form $\begin{bsmallmatrix}
			E \\ & 0
		\end{bsmallmatrix}$ where $E\in\GL_d(\FF_q)$. (The exact choice of $D_1$ and $D_2$ will not matter.)
		\item If $G\in\{\GSp_{2n},\Sp_{2n}\}$ so that $D$ is symmetric, finding an orthogonal basis grants $D_1\in\SL_s(\FF_q)$ such that $D_2\coloneqq D_1^\intercal$ has $D_1DD_2=\begin{bsmallmatrix}
			E \\ & 0
		\end{bsmallmatrix}$ where $E\in\GL_d(\FF_q)$ is diagonal.
		\item If $G\in\{\GO_{2n},\O_{2n}\}$ so that $D$ is alternating, theory about alternating forms grants a normal form with respect to a chosen basis (namely, we can make $D$ into the standard symplectic form together with some maximal isotropic subspace). As before, we are really being granted $D_1\in\SL_s(\FF_q)$ such that $D_2\coloneqq D_1^\intercal$ has $D_1DD_2=\begin{bsmallmatrix}
			E \\ & 0
		\end{bsmallmatrix}$ where $E=\begin{bsmallmatrix}
			& -1 \\ 1
		\end{bsmallmatrix}\in\FF_q^{d\times d}$.
	\end{itemize}
	Then we see that the matrix of interest is
	\[\underbrace{\begin{bmatrix}
		1_{n-s} \\ & D_2 \\ && 1_{n-s} \\ &&& D_1^{-1}
	\end{bmatrix}}_{\in G}\begin{bmatrix}
		&& 1_{n-s} \\ & 1_s \\
		\varepsilon1_{n-s} &   &   \\ & D_1DD_2 &   & 1_s
	\end{bmatrix}\underbrace{\begin{bmatrix}
		1_{n-s} \\ & D_2^{-1} \\ && 1_{n-s} \\ &&& D_1
	\end{bmatrix}}_{\in G},\]
	reducing ourselves from $D$ to $D_1DD_2=\begin{bsmallmatrix}
		E \\ & 0
	\end{bsmallmatrix}$: once again, $\chi$ will be trivial on the left and right matrices, so they do not matter for the computation for $f_r$. We now note that $\begin{bsmallmatrix}
		1 \\ E & 1
	\end{bsmallmatrix}=\begin{bsmallmatrix}
		-\varepsilon E^{-1} & 1 \\ & E
	\end{bsmallmatrix}\begin{bsmallmatrix}
		& \varepsilon1_d \\ 1
	\end{bsmallmatrix}\begin{bsmallmatrix}
		1 & E^{-1} \\ & 1
	\end{bsmallmatrix}$, so
	\[\begin{bmatrix}
		&&& 1_{n-s} \\ & 1_d \\ && 1_{n-r} \\
		\varepsilon1_{n-s} \\ & E &&& 1_d \\ && 0_{n-r} &&& 1_{n-r}
	\end{bmatrix}\]
	equals
	\[\begin{bmatrix}
		1_{n-s} \\ & -\varepsilon E^{-1} &&& 1_d \\ && 1_{n-r} \\
		&&& 1_{n-s} \\ &&&& E \\ &&&&& 1_{n-r}
	\end{bmatrix}\begin{bmatrix}
		&&& 1_{n-s} \\ &&&& \varepsilon1_d \\ && 1_{n-r} \\
		\varepsilon1_{n-s} \\ & 1_d \\ && 0_{n-r} &&& 1_{n-r}
	\end{bmatrix}\begin{bmatrix}
		1_{n-s} \\ & 1_d &&& E^{-1} \\ && 1_{n-r} \\
		&&& 1_{n-s} \\ &&&& 1_d \\ &&&&& 1_{n-r}
	\end{bmatrix}.\]
	Quickly, we note that the right matrix is in $U$ (in all cases for $G$, essentially by construction of $E$), and so it is trivial under $\chi$. We also note the middle matrix is in $G$, so the left matrix is in $G$ too, and we can see visually that it is in $P$.
    % Analogously, the left matrix is in $P$ in all cases for $G$: if $G\in\{\GL_{2n},\SL_{2n},\GSp_{2n},\Sp_{2n}\}$, then $\varepsilon=-1$, so $-\varepsilon E^{-1}=E^{-1}$, allowing us to check that the given matrix is in $E$; otherwise, if $G\in\{\GO_{2n},\O_{2n}\}$, then $\varepsilon=1$, so $-\varepsilon E^{-1}=-E^{-1}=E=E^{-\intercal}$, so we are still in $G$.
    Note that it is possible that the left matrix is nontrivial when passed through $\chi$; in fact, we will receive a contribution of $\beta(\det E)^{-1}$. (The multiplier is still trivial.)
	
	To compute the contribution of this element, it remains to transform the middle matrix into $\eta_r$. To begin, we note that this middle matrix equals
	\[\begin{bmatrix}
		&&& \varepsilon1_{n-s} \\ &&&& \varepsilon1_d \\ && 1_{n-r} \\
		1_{n-s} \\ & 1_d \\ &&&&& 1_{n-r}
	\end{bmatrix}\begin{bmatrix}
		\varepsilon1_{n-s} \\ & 1_d \\ && 1_{n-r} \\
		&&& \varepsilon1_{n-s} \\ &&&& 1_d \\ &&&&& 1_{n-r}
	\end{bmatrix},\]
	so this right matrix produces a contribution of $\beta(\varepsilon)^{n-s}$. Lastly, the left matrix equals
	\[\begin{bmatrix}
		&& \varepsilon1_r \\ & 1_{n-r} \\
		1_r \\ &&& 1_{n-r}
	\end{bmatrix}=\begin{bmatrix}
		& 1_{r} \\ 1_{n-r} \\
		&&& 1_{r} \\ && 1_{n-r}
	\end{bmatrix}\underbrace{\begin{bmatrix}
		1_{n-r} \\ &&& \varepsilon1_r \\
		&& 1_{n-r} \\ & 1_r
	\end{bmatrix}}_{\eta_r}\begin{bmatrix}
		& 1_{n-r} \\ 1_{r} \\
		&&& 1_{n-r} \\ && 1_{r}
	\end{bmatrix}.\]
	The left and right matrices are inverses of each other, so their contributions via $\chi$ will cancel out. Totaling our contributions from this $u\in U$, we achieve $\beta(\varepsilon)^{n-s}\beta(\det E)^{-1}$; summing over all $u$ completes the proof.
\end{proof}
\begin{remark}
	Consider $G\in\{\GL_{2n},\SL_{2n}\}$ with $\beta\ne1$. In this case, we can see that the value of $\det E$, even up to squares, fails to be well-defined given $D$ for most values of $r$ and $s$, so the sum doesn't even make sense! This corresponds to the fact that we tend to have $f_r=0$ for most $r$. A similar phenomenon can be seen for the other groups.
\end{remark}
\begin{remark} \label{rem:i-on-fj}
	We explain why we will essentially ignore what happens if we want to compute $If_r^J(\eta_s)$, where $f_r^J\in\left(\Ind_P^G\chi^J\right)^{\chi}$ is the usual basis vector (of \Cref{prop:twisted-ind-basis}). If $\beta^2\ne1$, then we appeal to \Cref{prop:generic-intertwining}. Otherwise, $\beta^2=1$. Now, one could compute as in the above proof, but observe that the entire proof only works with matrices with multiplier $1$, so there is no real chance for $\alpha$ to have any effect. But then $\chi$ and $\chi^J$ are equal on matrices with multiplier $1$ because $\beta=\beta^{-1}$, so we may as well have $f_r=f_r^J$! In particular, the answer is the same!
\end{remark}
We are now in a position to write down some matrices when $\beta^2=1$. We begin with the case where $\beta=1$ because it is a little simpler.
\begin{proposition} \label{prop:trivial-matrix-coeffs}
	Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Assume that $\beta=1$ so that $\chi=\chi^J$. Give $\left(\Ind_P^G\chi\right)^\chi$ the standard basis $\{f_0,\ldots,f_n\}$ of \Cref{prop:ind-irred}. For each $r,s\in\{0,\ldots,n\}$, define
	\[I(r,s)\coloneqq\begin{cases}
		\displaystyle(-1)^{r+s-n}\frac{(q;q)_s^2}{(q;q)_{n-r}^2(q;q)_{r+s-n}}q^{n^2-s^2+\binom{r+s-n}2} & \text{if }G\in\{\GL_{2n},\SL_{2n}\}, \\
		\displaystyle(-1)^{(r+s-n)/2}\frac{(q;q)_s}{(q;q)_{n-r}(q^2;q^2)_{(r+s-n)/2}}q^{\binom{n}2-\binom{s}2+2\binom{(r+s-n)/2}2} & \text{if }G\in\{\GO_{2n},\O_{2n}\}, \\
		\displaystyle(-1)^{r+s-n-\floor{(r+s-n)/2}}\frac{(q;q)_s}{(q;q)_{n-r}(q^2;q^2)_{\floor{(r+s-n)/2}}}q^{\binom{n+1}2-\binom{s+1}2+2\binom{\floor{(r+s-n)/2}+1}2} & \text{if }G\in\{\GSp_{2n},\Sp_{2n}\}, \\
	\end{cases}\]
	where we implicitly take $I(r,s)=0$ unless $r+s-n$ is nonnegative (and $I(r,s)=0$ unless $r+s-n$ is also even when $G\in\{\GO_{2n},\O_{2n}\}$). Then $[I(r,s)]_{0\le s,r\le n}$ is the matrix representation of $I$.
\end{proposition}
\begin{proof}
	We use \Cref{lem:matrix-coeff}, where the point is that $\beta=1$, so we are now just counting the number of possible $D$. Namely, the $(s,r)$ matrix coefficient is given by $If_r(\eta_s)$ because $f_r$ is the $r$th basis vector of the source, and plugging in $\eta_s$ detects the value of the coefficient for the $s$th target basis vector $f_s$. We now do our casework.
	\begin{itemize}
		\item Take $G\in\{\GL_{2n},\SL_{2n}\}$. Then we are counting matrices $D\in\FF_q^{s\times s}$ of rank $r+s-n$, which is
		\[q^{\binom{r+s-n}2}\cdot\frac{(q;q)_s^2}{(q;q)_{n-r}^2}\cdot\frac{(-1)^{r+s-n}}{(q;q)_{r+s-n}}\]
		by \cite[Theorem~7.1.5]{hach-gf}. Plugging this in and simplifying completes the computation.
		\item Take $G\in\{\GO_{2n},\O_{2n}\}$. Then we are counting alternating matrices $D\in\FF_q^{s\times s}$ of rank $r+s-n$, which \cite[Theorem~7.5.5]{hach-gf} explains equals
		\[q^{2\binom{(r+s-n)/2}2}\cdot\frac{1}{(-1)^{(r+s-n)/2}(q^2;q^2)_{r+s-n}}\cdot\frac{(-1)^s(q;q)_s}{(-1)^{n-r}(q;q)_{n-r}}\]
		provided that $r+s-n$ is even and nonnegative (else is $0$). Plugging this in and simplifying completes.
		\item Take $G\in\{\GSp_{2n},\Sp_{2n}\}$. Then we are counting symmetric matrices $D\in\FF_q^{s\times s}$ of rank $r+s-n$, which \cite[Theorem~7.5.2]{hach-gf} explains equals
		\[q^{2\binom{\floor{(r+s-n)/2}}2}\cdot\frac{1}{(-1)^{\floor{(r+s-n)/2}}(q^2;q^2)_{\floor{(r+s-n)/2}}}\cdot\frac{(-1)^s(q;q)_s}{(-1)^{n-r}(q;q)_{n-r}}.\]
		Plugging this in and simplifying completes.
		\qedhere
	\end{itemize}
\end{proof}
Lastly, we address the case where $\beta^2=1$ but $\beta\ne1$. Again, because it is simplest, we handle $G\in\{\GL_{2n},\SL_{2n}\}$ first.
\begin{proposition} \label{prop:quad-gl-sl-matrix}
	Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Assume that $\beta^2=1$ but $\beta\ne1$.
	\begin{itemize}
		\item If $G=\GL_{2n}$, then we give $\left(\Ind_P^G\chi\right)^\chi$ the basis $\{f_0\}$ of \Cref{prop:ind-irred} and give $\left(\Ind_P^G\chi^J\right)^{\chi}$ the basis $\{f_n^J\}$ of \Cref{prop:twisted-ind-basis}. Then we compute 
		\[If_0=f_n^J\qquad\text{and}\qquad If_n^J=\beta(-1)^nq^{n^2}f_0.\]
		\item If $G=\SL_{2n}$, then we see $\chi=\chi^J$, so we give $\left(\Ind_P^G\chi\right)^\chi$ the basis $\{f_0,f_n\}$ of \Cref{prop:ind-irred}. Then $I$ has the matrix representation
		\[\begin{bmatrix}
			& \beta(-1)q^{n^2} \\ 1
		\end{bmatrix}.\]
	\end{itemize}
\end{proposition}
\begin{proof}
	It suffices to compute $If_0\in\op{span}\left\{f_0^J,f_n^J\right\}$ and $If_n^J\in\op{span}\{f_0,f_n\}$ in both cases. Note $If_0(\eta_0)=0$ because the sum in \Cref{lem:matrix-coeff} vanishes; as a small trick, the fact that $f_n$ vanishes for $G=\GL_{2n}$, we see that the sum for $If_n^J(\eta_n)$ must vanish. (One could also compute this sum by hand in the case $G=\SL_{2n}$: it is $\sum_{D\in\op{GL}_s(\FF_q)}\beta(\det D)$, which vanishes because $\beta\circ\det$ is a nontrivial character on the group $\GL_s(\FF_q)$.)

	So it remains to compute $If_0=If_0(\eta_n)f_n^J$ and $If_n^J=If_n^J(\eta_0)f_0$, which are the expected computations from the $G=\GL_{2n}$ case. We handle the computations separately.
	\begin{itemize}
		\item We compute $If_0(\eta_n)$. Here, \Cref{lem:matrix-coeff} wants us to sum $D\in\FF_q^{n\times n}$ of rank $0$. As such, we are only looking at $D=0$, for which the sum returns $1$. Because $\beta(\varepsilon)^{n-n}=Q=1$ too, we see $If_0(\eta_n)=1$.
		\item We compute $If_n^J(\eta_0)$. We may still use \Cref{lem:matrix-coeff} for our computation by \Cref{rem:i-on-fj}. This time, we are summing $D\in\FF_q^{0\times0}$ of rank $0$, of which there is exactly one invertible matrix of determinant $1$ (by convention), so the sum still returns $1$. Thus, we find $If_n^J(\eta_0)=\beta(\varepsilon)^nq^{n^2}$.
	\end{itemize}
	Using the above computations to build matrices completes the proof.
\end{proof}
\begin{remark}
	\Cref{prop:quad-gl-sl-matrix} provides our first nontrivial example where we can see that $I$ is diagonalizable. Namely, for $G=\SL_{2n}$, the characteristic polynomial of $I$ is $X^2-\beta(-1)q^{n^2}$, so $I$ has distinct eigenvalues $\pm\sqrt{\beta(-1)}q^{n^2/2}$.
\end{remark}
It remains to handle $G\in\{\GO_{2n},\O_{2n},\GSp_{2n},\Sp_{2n}\}$. Though it was a little troublesome for $G=\GL_{2n}$, this will be the first time when we will really have to deal with the complication $\chi\ne\chi^J$ for $G\in\{\GL_{2n},\GO_{2n},\GSp_{2n}\}$. Luckily, our bases are well-behaved enough so that this is not really a problem.
\begin{lemma} \label{lem:general-from-special-matrix}
	Take $G\in\{\GO_{2n},\GSp_{2n}\}$ so that $S\coloneqq\ker m$ is in $\{\O_{2n},\Sp_{2n}\}$. Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Assume that $\beta^2=1$ but $\beta\ne1$. Let $\left\{f_r^G\right\}_{r\equiv_20}$ be the usual basis of $\left(\Ind_P^G\chi\right)^\chi$, let $\left\{f_r^{JG}\right\}_{r\equiv_2 n}$ be the usual basis of $\left(\Ind_P^G\chi^J\right)^{\chi}$, and define $f_r^S$ similarly for $\chi|_S$. Let $\left[I^S(r,s)\right]_{0\le s,r\le n}$ be the matrix representation of $I$ on $\left(\Ind_P^G\chi\right)^\chi$.
	\begin{itemize}
		\item The matrix representation of $I\colon\left(\Ind_G^P\chi\right)^\chi\to\left(\Ind_G^P\chi^J\right)^\chi$ is
		\[\left[I^S(r,s)\right]_{\substack{0\le s,r\le n\\r\equiv_20,s\equiv_2 n}}.\]
		\item The matrix representation of $I\colon\left(\Ind_G^P\chi^J\right)^\chi\to\left(\Ind_G^P\chi\right)^\chi$ is
		\[\left[I^S(r,s)\right]_{\substack{0\le s,r\le n\\r\equiv_2n,s\equiv_2 0}}.\]
	\end{itemize}
\end{lemma}
\begin{proof}
	The proofs of the two points are essentially the same (though one should use \Cref{rem:i-on-fj} for the second point), so we focus on the first one. As in \Cref{prop:trivial-matrix-coeffs}, we see that the $(s,r)$ coefficient of $I$ will be the coefficient of $f_s^J$ in $If_r$. So we note that the computation of \Cref{lem:matrix-coeff} explains that the value of $If_r(\eta_s)$ is the same no matter if we work with $G$ or $S$, so our answer is $I^S(r,s)$, as required.
\end{proof}
\begin{remark}
	Similarly, one can see that the matrix representation of $I$ acting on the spaces $\left(\Ind_G^P\chi^J\right)^{\chi^J}$ and $\left(\Ind_G^P\chi\right)^{\chi^J}$ are the corresponding submatrices of $\left[I^S(r,s)\right]_{0\le s,r\le n}$.
\end{remark}
\begin{remark}
	One can see a version of the above result still hold for $\SL_{2n}\subseteq\GL_{2n}$ in the sense that the matrices for $\GL_{2n}$ are submatrices of $\SL_{2n}$ essentially dictated by which basis vectors are in play.
\end{remark}
\begin{remark}
	Here is a cute application of the above result, akin to the argument that $If_n^J(\eta_n)=0$ in \Cref{prop:quad-gl-sl-matrix}. Using the notation of the lemma above, we will show that $I^S(r,s)=0$ if $r+s\not\equiv n\pmod 2$ and one of $r$ or $n-r$ is even. In the case that $r$ is even, we simply note that $f_r^G$ is nonzero, so \Cref{lem:matrix-coeff} informs us that
	\[I^S(r,s)=I^Sf_r^S(\eta_s)=I^Gf_r^G(\eta_s)=0,\]
	where the last equality holds because $f_s^{JG}$ is zero. In the case that $n-r$ is even, we run the same argument but replace $f_r^G$ with $f_r^{JG}\ne0$ and replace $f_s^{JG}$ with $f_s$.
\end{remark}
\Cref{lem:general-from-special-matrix} tells us that we can essentially focus on $G\in\{\O_{2n},\Sp_{2n}\}$. For example, the eigenvalues of $I\circ I$ in the case that $G\in\{\GO_{2n},\GSp_{2n}\}$ can be read off the eigenvalues by taking suitable submatrices everywhere. Anyway, we now handle $G\in\{\O_{2n},\Sp_{2n}\}$. The approaches are rather different, so we will handle them separately.
\begin{proposition}
	Take $G=\O_{2n}$. Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=\beta\circ\chi_{\det}$. Assume that $\beta^2=1$ but $\beta\ne1$ so that $\chi=\chi^J$. Let $\{f_0,\ldots,f_n\}$ be the standard basis of $\left(\Ind_P^G\chi\right)^\chi$. Then the matrix representation of $I$ is
	\[\left[(-1)^{(r+s-n)/2}q^{\binom{n}2-\binom{s}2+2\binom{(r+s-n)/2}2}\frac{(q;q)_s}{(q;q)_{n-r}(q^2;q^2)_{(r+s-n)/2}}\right]_{0\le s,r\le n},\]
	where the coefficient implicitly vanishes unless $r+s-n$ is a nonnegative even integer.
\end{proposition}
\begin{proof}
	Arguing as in \Cref{prop:trivial-matrix-coeffs}, we use \Cref{lem:matrix-coeff}. In \Cref{prop:trivial-matrix-coeffs}, evaluating the sum in \Cref{lem:matrix-coeff} was relatively easy because $\beta$ was trivial, implying $\beta(\det E)^{-1}=1$ always. But in the case that $G=\O_{2n}$, we see that $\det E=1$ by definition of $E$, so we still have $\beta(\det E)^{-1}=1$. Thus, the argument of \Cref{prop:trivial-matrix-coeffs} goes through, with the caveat that we must consider the sign $\beta(\varepsilon)^{n-s}=\beta(1)^{n-s}=1$ which occurs when evaluating $If_r(\eta_s)$.
\end{proof}
\begin{proposition} \label{prop:sp-quadratic-matrix}
	Take $G=\Sp_{2n}$. Fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=\beta\circ\chi_{\det}$. Assume that $\beta^2=1$ but $\beta\ne1$ so that $\chi=\chi^J$. Let $\{f_0,\ldots,f_n\}$ be the standard basis of $\left(\Ind_P^G\chi\right)^\chi$. If $\beta(-1)=1$, then the matrix representation of $I$ is
	\[\left[\beta(-1)^{n-s+(r+s-n)/2}(-1)^{(r+s-n)/2}q^{\binom{n+1}2-\binom{s+1}2+2\binom{(r+s-n)/2+1}2-(r+s-n)/2}\frac{(q;q)_s}{(q;q)_{n-r}(q^2;q^2)_{(r+s-n)/2}}\right]_{0\le s,r\le n},\]
	where the coefficient implicitly vanishes unless $r+s-n$ is a nonnegative even integer.
\end{proposition}
\begin{proof}
	Arguing as in \Cref{prop:trivial-matrix-coeffs}, we use \Cref{lem:matrix-coeff}. In \Cref{prop:trivial-matrix-coeffs}, evaluating the sum in \Cref{lem:matrix-coeff} was relatively easy because $\beta$ was trivial. This time around, we see that we are taking the difference between the number of symmetric $D\in\FF_q^{s\times s}$ of rank $r+s-n$ with $\det E\in\FF_q^{\times2}$ and the number of $D$ with $\det E\notin\FF_q^{\times2}$. The formulae of \cite{macwilliams-ortho-matrices} tell us that the number of such $D$ with $\det E\in\FF_q^{\times2}$ is
	\[\begin{cases}
		\frac12N & \text{if }r+s-n\equiv1\pmod2, \\
		\frac12N\cdot\frac{q^{(r+s-n)/2}+\beta(-1)^{(r+s-n)/2}}{q^{(r+s-n)/2}} & \text{if }r+s-n\equiv0\pmod2,
	\end{cases}\]
	where $N$ is the total number of symmetric matrices $D\in\FF_q^{s\times s}$ of rank $r+s-n$. Thus, we see that the desired difference is $0$ if $r+s-n$ is odd and $\beta(-1)^{(r+s-n)/2}q^{-(r+s-n)/2}N$ otherwise. Arguing as in \Cref{prop:trivial-matrix-coeffs} to extract the desired matrix coefficient from \Cref{lem:matrix-coeff} completes the proof.
\end{proof}

% In the light of the previous lemma, we spend the rest of our time with $G\in\{\SL_{2n},\O_{2n},\Sp_{2n}\}$ and $\beta^2=1$. Here, $\alpha=1$ always, so we are actually in the case where $\chi^2=1$ and $\chi=\chi^J$. Thus, $I$ is actually an operator on $\Ind^G_P\chi$.\footnote{Note that $I$ will not necessarily be an operator on $\Ind^G_P\chi$ when $G\in\{\GL_{2n},\GO_{2n},\GSp_{2n}\}$ even if $\beta^2=1$, which is more or less why we made the above reduction; this will come up again later on.\todo{When?}} 

% compute MJ on the basis of I(chi,chi') with lots of casework
% state the eigenvalues of the matrix operators
% (it should be the case that either our rep is irreducible, or MJ is an operator)
% the hecke Z-algebra for B\G/B (attached to the weyl group and coxeter data, etc.) is finite over Z[q], and MJ lives there. so MJ is going to be the root of some polynomial in Z[q][T]; the point here is that we expeect to write the eigenvalues of MJ as "reasonable" functions of q (they are going to be roots of some polynomial in Z[q][T] of bounded degree, granting some uniformity in q)
% I suspect one can define the subalgebra on P\G/P still over Z[q], perhaps by embedding in the larger Hecke algebra or perhaps by explicitly writing down relations (note this algebra is commutative); this should improve the bound on degree of the eigenvalues
% do also note that theory of hecke algebras promises that MJ is invertible immediately (e.g., this follows from the quadratic relation; namely, MJ is invertible on the Borel induction!)
% it may be worth writing out the structure of the hecke algebra on P\G/P in terms of our basis (computing some convolutions), but expressing MJ in this basis is pretty horrendous; maybe see T32.1 of Bump

% what is remarkable is that the eigenvalues of such small degree over Q(q) (either one or two)
% I have no idea why this should be true a priori; it is likely somethin special about MJ ...

\subsection{A Multiplicity One Result}
Our understanding of $I$ so far has relied on eigenvectors of $\Ind_P^G\chi$ with eigenvalue $\chi$ (or $\chi^J$). The eigenvector we are interested in constructing will be an eigenvector for the smaller subgroup $U\subseteq P$. As such, we are interested in building some characters for $U$.
\begin{definition}
	Fix $T\in \FF_q^{n\times n}$ and a character $\psi\colon\FF_q\to\CC^\times$. Then we define the character $\psi_T\colon U\to\CC$ by
	\[\psi_T\left(\begin{bmatrix}
		1_n & B \\ & 1_n
	\end{bmatrix}\right)\coloneqq\psi(\tr BT).\]
\end{definition}
We are interested in $\psi_T$-eigenvectors of $\Ind_P^G\chi$. As such, we begin by showing that such eigenvectors exist.
\begin{example} \label{ex:produce-psi-t-eigen}
	Fix $T\in \FF_q^{n\times n}$ and a character $\psi\colon\FF_q\to\CC^\times$. Given a character $\chi\colon P\to\CC^\times$, define $f_T\in\Ind_G^P\chi$ to be supported on $P\eta_nP$ and defined by
	\[f_{\chi,T}(p\eta_nu)\coloneqq\chi(p)\psi_T(u).\]
	This will be a perfectly fine definition of a nonzero element of $\left(\Ind_G^P\chi\right)^{\psi_T}$ as soon as we can show that any $g\in P\eta_nP$ can be uniquely expressed as $p\eta_nu$ where $p\in P$ and $u\in U$. On one hand, recall that $\eta_n=J$ normalizes $M$ (see \Cref{def:chi-j}), so
	\[P\eta_nP=P\eta_nMU=P\eta_nM\eta_n^{-1}\eta_nU=PM\eta_nU=P\eta_nU,\]
	so any $g\in P\eta_nP$ can be expressed as $p\eta_nu$ for $p\in P$ and $u\in U$. It remains to show that this expression is unique: if $p_1\eta_nu_1=p_2\eta_nu_2$, then $\eta_nu\eta_n^{-1}\in P$ for $u\coloneqq u_2u_1^{-1}$, so writing $u\coloneqq\begin{bsmallmatrix}
		1 & B \\ & 1
	\end{bsmallmatrix}$, we see
	\[\begin{bmatrix}
		& \varepsilon1_n \\ 1_n
	\end{bmatrix}\begin{bmatrix}
		1_n & B \\ & 1_n
	\end{bmatrix}\begin{bmatrix}
		& 1_n \\ \varepsilon1_n
	\end{bmatrix}=\begin{bmatrix}
		1_n \\ \varepsilon B & 1_n
	\end{bmatrix}.\]
	For this to be in $P$, we see that we must have $u=1_{2n}$, which then implies $u_1=u_2$ and so $p_1=p_2$.
\end{example}
Here is our main result of the present subsection.
\begin{proposition} \label{prop:psi-t-mult-one}
	Fix $T\in\GL_n(\FF_q)$ such that $\begin{bsmallmatrix}
		1 & T \\ & 1
	\end{bsmallmatrix}\in G$ and a nontrivial character $\psi\colon\FF_q\to\CC^\times$. For any character $\chi\colon P\to\CC^\times$, we have
	\[\dim\op{Hom}_U\left(\psi_T,\Ind_P^G\chi\right)=1.\]
	In other words, $\op{Hom}_U\left(\psi_T,\Ind_P^G\chi\right)$ is spanned by the $f_{\chi,T}$ of \Cref{ex:produce-psi-t-eigen}.
\end{proposition}
\begin{proof}
	We use Mackey theory. By Frobenius reciprocity, we are computing the dimension of the space $\Hom_G\left(\Ind_U^G\psi_T,\Ind_P^G\chi\right)$, which \cite[Theorem~32.1]{bump-lie-group} explains equals the dimension of
	\[\mc H\coloneqq\left\{f\in\op{Mor}(G,\CC):f(pgu)=\chi(p)f(g)\psi_T(u)\text{ for }p\in P,g\in G,u\in U\right\}.\]
	We proceed in steps.
	\begin{enumerate}
		\item We see that we are interested in the double coset space $P\backslash G/U$. \Cref{lem:compute-pgp} tells us that any $g\in G$ can be written as $p_1\eta_rp_2$ for some $p_1,p_2\in P$ and $r\in\{0,\ldots,n\}$. Because $P/U\cong M$, we thus see that any $g\in G$ can be written as $p\eta_rdu$ for $p_1\in P$ and $r\in\{0,\ldots,n\}$ and $d\in M$ and $u\in U$. Thus, $f\in\mc H$ is determined by its values on $f(\eta_rd)$ for $r\in\{0,\ldots,n\}$ and $d\in M$. (We are not claiming that these are unique double coset classes, but it will not usually be significant.)

		% \item To begin, we check that $\mc H$ is actually nonzero: simply define $f_T\colon G\to\CC$ to be supported on $P\eta_nP$ and defined by
		% \[f_T(p\eta_nu)\coloneqq\chi(p)\psi_T(u).\]
		% This will be a perfectly fine definition of a nonzero element of $\mc H$ as soon as we can show that any $g\in P\eta_nP$ can be uniquely expressed as $p\eta_nu$ where $p\in P$ and $u\in U$. On one hand, recall that $\eta_n=J$ normalizes $M$ (see \Cref{def:chi-j}), so
		% \[P\eta_nP=P\eta_nMU=P\eta_nM\eta_n^{-1}\eta_nU=PM\eta_nU=P\eta_nU,\]
		% so any $g\in P\eta_nP$ can be expressed as $p\eta_nu$ for $p\in P$ and $u\in U$. It remains to show that this expression is unique: if $p_1\eta_nu_1=p_2\eta_nu_2$, then $\eta_nu\eta_n^{-1}\in P$ for $u\coloneqq u_2u_1^{-1}$, so writing $u\coloneqq\begin{bsmallmatrix}
		% 	1 & B \\ & 1
		% \end{bsmallmatrix}$, we see
		% \[\begin{bmatrix}
		% 	& \varepsilon1_n \\ 1_n
		% \end{bmatrix}\begin{bmatrix}
		% 	1_n & B \\ & 1_n
		% \end{bmatrix}\begin{bmatrix}
		% 	& 1_n \\ \varepsilon1_n
		% \end{bmatrix}=\begin{bmatrix}
		% 	1_n \\ \varepsilon B & 1_n
		% \end{bmatrix}.\]
		% For this to be in $P$, we see that we must have $u=1_{2n}$, which then implies $u_1=u_2$ and so $p_1=p_2$.

		\item For the remainder of the proof, our goal will be to show that $f\in\mc H$ will have $f(\eta_rd)=0$ for any $d\in M$ whenever $r\ne n$. This will complete the proof because it shows that any $f\in\mc H$ is supported on $P\eta_nP=P\eta_nU$, meaning that $f=f(\eta_n)f_{\chi,T}$, so $\left\{f_{\chi,T}\right\}$ is a basis of $\mc H$. (Here, $f_{\chi,T}$ is defined from \Cref{ex:produce-psi-t-eigen}.)

		The basic sketch is that we will find various $u\in U$ such that $\eta_rdu=p\eta_rd$ for some $p\in P$, which will allow us to show that $f(\eta_rd)=f(\eta_rdu)$, but then $f(\eta_rd)\ne0$ requires $\psi_T(u)=1$. Having enough $u$ will allow us to force a full column of $T$ to vanish, violating the hypothesis that $T$ is invertible.

		% \item Call $m\in \FF_q^{n\times n}$ ``sparse'' if $mv=0$ or $v^\intercal m=0$ for all $v\in\FF_q^n$. As a small technical lemma, we claim that $m$ being sparse implies that $g^{-1}mg$ is sparse for any $g\in\GL_n(\FF_q)$ satisfying $g=g^{-1}$. Indeed, for any $v\in\FF_q^n$, we see that $m(gv)=0$ or 

		\item Fix some $\eta_r$ and $d\in M$. If $\eta_rdu=p\eta_rd$ for some $u\in U$ and $p\in P$, then we claim $\chi(p)=1$. In other words, we are showing that $\chi$ is trivial on any $p\in P\cap\eta_rdUd^{-1}\eta_r^{-1}$. Quickly, note that $M$ normalizes $U$, so $dUd^{-1}=U$, so we may as well assume that $d=1_{2n}$.

		Now, we are given $u\in U$ such that $p\coloneqq\eta_ru\eta_r^{-1}$ is in $P$, and we want to show that $\chi(p)=1$. Well, we expand $u$ using block matrices to see that
		\begin{align*}
			\eta_ru\eta_r^{-1} &= \begin{bmatrix}
				1_{n-r} \\ &&& \varepsilon1_r \\
				&& 1_{n-r} \\ & 1_r
			\end{bmatrix}\begin{bmatrix}
				1_{n-r} && A & B \\ & 1_r & C & D \\
				&& 1_{n-r} \\ &&& 1_r
			\end{bmatrix}\begin{bmatrix}
				1_{n-r} \\ &&& 1_r \\
				&& 1_{n-r} \\ & \varepsilon1_r
			\end{bmatrix} \\
			&= \begin{bmatrix}
				1_{n-r} & \varepsilon B & A \\ & 1_r \\
				&& 1_{n-r} \\ & \varepsilon D & C & 1_r
			\end{bmatrix}.
		\end{align*}
		For this to be in $P$, we see that $D=0$. We then see that the resulting matrix has multiplier $1$ and Siegel determinant $1$, so $\chi$ is trivial on it.

		\item Fix some $\eta_r$ and $d\in M$ such that $r<n$. We claim that there exists $u\in U$ such that $\psi_T(u)\ne1$ and $\eta_rdu=p\eta_rd$ for some $p\in P$. We will proceed more or less by contraposition: we show that having $\psi_T(u)=1$ for all such $u$ will imply that $T$ fails to be invertible. Observe that this is the only step of the proof which will use that $T$ is supposed to be invertible and that $\psi$ is nontrivial.

		The condition on $u\in U$ is that $\eta_rdud^{-1}\eta_r^{-1}\in P$. Again, because $M$ normalizes $U$, our hypothesis is simply that $\eta_ru\eta_r^{-1}\in P$ implies $\psi_T\left(d^{-1}ud\right)=1$; by replacing $T$ with $dTd^{-1}$ (which does not change whether $T$ is invertible), we see that we may assume $d=1_{2n}$. Using the computation in the previous step, we see that $\eta_ru\eta_r^{-1}\in P$ is equivalent to $D=0$. But with $r>0$, we thus see that we are permitted to use many $u\in U$; we will use the subgroup
		\[U_1\coloneqq\left\{\begin{bmatrix}
			1 & B \\ & 1
		\end{bmatrix}:B_{ij}=0\text{ for }i,j>1\right\}.\]
		Namely, it remains to show that $\psi_T$ being trivial on $U_1$ implies that $T$ fails to invertible. In fact, we will show that $Te_1=0$, which will finish the proof. Quickly, we note that $\psi_T(u)=1$ for $u$ in the above form is simply asserting
		\[1\stackrel?=\psi(\tr BT)=\sum_{i=1}^n\psi((TB)_{ii})=\sum_{i,j=1}^n\psi(T_{ij}B_{ji})=\psi(T_{11}B_{11})+\sum_{i=2}^n\psi(T_{i1}B_{1i})+\sum_{j=2}^n\psi(T_{1j}B_{j1}).\]
		To continue, we will do some casework on $G$.
		\begin{itemize}
			\item Take $G\in\{\GL_{2n},\SL_{2n}\}$. Then we may set the $B_{ij}$ arbitrarily, provided that $i=1$ or $j=1$. We would like to show that $T_{i1}=0$ for all $i$, so fixing some $i$, we set all coordinates except $B_{1i}$ to zero so that we know $\psi(T_{i1}B_{1i})=1$ for all $B_{1i}\in\FF_q$. Now, if $T_{i1}$ were nonzero, then left multiplication by $T_{i1}$ would be a surjective map $\FF_q\to\FF_q$, so we would be able to find some $B_{1i}$ such that $\psi(T_{i1}B_{1i})\ne1$ because $\psi$ is nontrivial. Thus, we must instead have $T_{1i}=0$.
			\item Take $G\in\{\GO_{2n},\O_{2n}\}$. Then both $T$ and $B$ must be alternating; for example, we get $T_{11}=B_{11}=0$ for free. Thus, it remains to show that $T_{i1}=0$ for all $i\ge2$. Again, we fix some $i$, and we set all coordinates except $B_{1i}$ and $B_{1i}=-B_{i1}$ to zero, so we see that $\psi(2T_{i1}B_{1i})=1$ for all $B_{1i}\in\FF_q$. Arguing as in the previous point, this implies $T_{i1}=0$, as required.
			\item Take $G\in\{\GSp_{2n},\Sp_{2n}\}$. Then both $T$ and $B$ must be symmetric. We want to show that $T_{i1}=0$ for all $i\ge1$. Again, we fix some $i$, and we set all coordinates except $B_{1i}$ and $B_{1i}=-B_{i1}$ to zero, so we see that $\psi(cT_{i1}B_{1i})=1$ for all $B_{1i}\in\FF_q$, where $c=1$ if $i=1$ and $c=2$ if $i>1$. Arguing as in the previous point, this still implies $T_{i1}=0$, as required.
		\end{itemize}

		\item We now complete the proof. Given some $f\in\mc H$, we would like to show that $f(\eta_rd)=0$ whenever $r<0$. Well, the previous step provides $p\in P$ and $u\in U$ such that $p\eta_rd=\eta_rdu$ and $\psi_T(u)\ne1$. But we know that any such $p$ must have $\chi(p)=1$, so the equation
		\[f(\eta_rd)=\chi(p)f(\eta_rd)=f(p\eta_rd)=f(\eta_rdu)=\psi_T(u)f(\eta_rd)\]
		forces $f(\eta_rd)=0$, as claimed.
		\qedhere
	\end{enumerate}
\end{proof}
\begin{remark} \label{rem:bad-orthogonal}
	It is possible for no $T$ satisfying the hypotheses of \Cref{prop:psi-t-mult-one} to exist! Namely, suppose $n$ is odd and $G\in\{\GO_{2n},\O_{2n}\}$. Then we are asking for $T$ to be an invertible $n\times n$ alternating matrix, but any alternating matrix has odd rank, so no such $T$ exists! However, we can find some $T$ in all other cases. For example, $T=1_{2n}$ works for $G\in\{\GL_{2n},\SL_{2n},\GSp_{2n},\Sp_{2n}\}$ for any $n$, and $T\coloneqq\op{diag}\left(\begin{bsmallmatrix}
		& -1 \\ 1
	\end{bsmallmatrix},\ldots,\begin{bsmallmatrix}
		& -1 \\ 1
	\end{bsmallmatrix}\right)$ works for $G\in\{\GO_{2n},\O_{2n}\}$ when $n$ is even.
\end{remark}
This multiplicity-one result means that we can gain insight into $I\colon\Ind_P^G\chi\to\Ind_P^G\chi^J$ by plugging in $f_{\chi,T}$. This will lead us to evaluate certain matrix Gauss sums. We begin by defining the relevant sums.
\begin{definition}
	Fix $T\in \FF_q^{n\times n}$ and characters $\beta\colon\FF_q^\times\to\CC^\times$ and $\psi\colon\FF_q\to\CC^\times$. Then we define the ``Gauss sum''
	\[g^G(\beta,\psi,T)\coloneqq\sum_{\substack{B\in\GL_n(\FF_q)\\\begin{bsmallmatrix}
		1 & B \\ & 1
	\end{bsmallmatrix}\in G}}\beta(\det B)\psi(\tr BT).\]
\end{definition}
And here is our result.
\begin{proposition} \label{prop:i-on-psi-eigen}
	Fix $T\in\GL_n(\FF_q)$ such that $\begin{bsmallmatrix}
		1 & T \\ & 1
	\end{bsmallmatrix}\in G$ and a nontrivial character $\psi\colon\FF_q\to\CC^\times$. Further, fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Then
	\[If_{\chi,T}=g^G(\beta,\psi,T)f_{\chi^J,T}.\]
\end{proposition}
\begin{proof}
	For brevity, let $\ov U$ denote the subgroup of $B\in \FF_q^{n\times n}$ such that $\begin{bsmallmatrix}
		1 & B \\ & 1
	\end{bsmallmatrix}\in G$, and we let $\ov U^\times$ denote the invertible subset. Note that $I$ carries $\psi_T$-eigenvectors to $\psi_T$-eigenvectors, so \Cref{prop:psi-t-mult-one} tells us that
	\[If_{\chi,T}=\left(If_{\chi,T}(\eta_n)\right)f_{\chi^J,T}.\]
	So it remains to evaluate $If_{\chi,T}(\eta_n)$. We will do this by direct computation. We write
	\[If_{\chi,T}(\eta_n)=\sum_{u\in U}f_{\chi,T}\left(\eta_n^{-1}u\eta_n\right).\]
	Now, writing $u=\begin{bsmallmatrix}
		1 & B \\ & 1
	\end{bsmallmatrix}$, we see that $\eta_n^{-1}u\eta_n=\begin{bsmallmatrix}
		1 \\ & \varepsilon B & 1
	\end{bsmallmatrix}$, so because $\ov U$ is an additive group, we see
	\[If_{\chi,T}(\eta_n)=\sum_{B\in\ov U}f_{\chi,T}\left(\begin{bmatrix}
		1_n \\ B & 1_n
	\end{bmatrix}\right).\]
	Now, $f_{\chi,T}$ is supported on $P\eta_nU=P\eta_nP$, so the proof of \Cref{lem:compute-pgp} tells us that $B\in\ov U$ produces a nonzero contribution if and only if $B$ is invertible. To compute this contribution, we note
	\[\begin{bmatrix}
		1_n \\ B & 1_n
	\end{bmatrix}=\begin{bmatrix}
		-\varepsilon B^{-1} & 1_n \\ & B
	\end{bmatrix}\begin{bmatrix}
		& \varepsilon1_n \\ 1_n
	\end{bmatrix}\begin{bmatrix}
		1_n & B^{-1} \\ & 1_n
	\end{bmatrix},\]
	and we see that the left and right matrices here do in fact live in $G$. For the right, we simply need to note that $B^{-1}\in\ov U$ still (namely, if $B$ is symmetric or alternating, then so is $B^{-1}$). Additionally, the middle matrix is in $G$, so the left matrix is also in $G$.\footnote{Alternatively, in the interesting cases when $G\notin\{\GL_{2n},\SL_{2n}\}$, we can directly compute $B^{-\intercal}=-\varepsilon B^{-1}$ so that $\begin{bsmallmatrix}
		-\varepsilon B^{-1} & 1 \\ & B
	\end{bsmallmatrix}=\begin{bsmallmatrix}
		B^{-\intercal} \\ & B
	\end{bsmallmatrix}\begin{bsmallmatrix}
		1 & B^\intercal \\ & 1
	\end{bsmallmatrix}$ is in $P$.} Thus, we conclude
	\[If_{\chi,T}=\sum_{B\in\ov U^\times}f_{\chi,T}\left(\begin{bmatrix}
		-\varepsilon B^{-1} & 1_n \\ & B
	\end{bmatrix}\begin{bmatrix}
		& \varepsilon1_n \\ 1_n
	\end{bmatrix}\begin{bmatrix}
		1_n & B^{-1} \\ & 1_n
	\end{bmatrix}\right)=\sum_{B\in\ov U^\times}\beta\left(\det B^{-1}\right)\psi_T\left(B^{-1}\right).\]
	Replacing $B$ with $B^{-1}$ completes the proof.
\end{proof}
Thus, we see that the values of $g^G(\omega,\psi,T)$ will be interesting to us. For example, in the cases where $\chi=\chi^J$, the above proposition tells us that $g^G(\beta,\psi,T)$ is an eigenvalue of $I$. In the general case when merely $I\circ I$ is an operator on $\Ind_G^P\chi$, we get the following.
\begin{corollary}
	Fix $T\in\GL_n(\FF_q)$ such that $\begin{bsmallmatrix}
		1 & T \\ & 1
	\end{bsmallmatrix}\in G$ and a nontrivial character $\psi\colon\FF_q\to\CC^\times$. Further, fix a character $\chi\colon P\to\CC^\times$, which we write as $\chi=(\alpha\circ m)(\beta\circ\chi_{\det})$. Then
	\[(I\circ I)f_{\chi,T}=\beta(-1)^n\left|g^G(\beta,\psi,T)\right|^2f_{\chi,T}.\]
\end{corollary}
\begin{proof}
	Applying \Cref{prop:i-on-psi-eigen} twice, we see that
	\[(I\circ I)f_{\chi,T}=g^G(\beta,\psi,T)g^G\left(\beta^{-1},\psi,T\right)f_{\chi,T}\]
	because $\left(\chi^J\right)^J=\chi$. In order to relate the above scalar to $\left|g^G(\beta,\psi,T)\right|^2$, we compute
	\begin{align*}
		\ov{g^G(\beta,\psi,T)} &= \sum_B\ov\beta(\det B)\ov\psi(\tr BT) \\
		&= \sum_B\beta^{-1}\left(\det B\right)\psi(\tr-BT) \\
		&= \sum_B\beta^{-1}(\det-B)\psi(\tr BT) \\
		&= \beta(-1)^ng^G\left(\beta^{-1},\psi,T\right).
	\end{align*}
	Thus, $g^G(\beta,\psi,T)g^G\left(\beta^{-1},\psi,T\right)=\beta(-1)^n\left|g^G(\beta,\psi,T)\right|^2$.
\end{proof}
\begin{remark}
	We take $\beta^2\ne1$, and we compare the above computation with \Cref{prop:generic-intertwining}. When $G\in\{\GL_{2n},\SL_{2n},\GSp_{2n},\Sp_{2n}\}$, we see that $\varepsilon=-1$, so it follows that
	\begin{equation}
		\left|g^G(\beta,\psi,T)\right|^2=\left|U\right|. \label{eq:gauss-sum-mag}
	\end{equation}
	When $G\in\{\GO_{2n},\O_{2n}\}$, it may appear that our signs may disagree, but recall from \Cref{rem:bad-orthogonal} that $T$ does not even exist when $n$ is odd, so we will always have $\beta(-1)^n=\beta(\varepsilon)^n$; thus, the above equation still holds. As such, we see that the sum in the definition of $g^G(\beta,\psi,T)$ obeys the expected ``square root'' cancellation generically.
\end{remark}
\begin{remark}
	Later on, we will compute the values of $g^G(\beta,\psi,T)$ in terms of usual Gauss sums; for example, we will be able to check \eqref{eq:gauss-sum-mag} directly. Do note that \eqref{eq:gauss-sum-mag} cannot hold in the non-generic cases where $\beta^2=1$ because $\left|g^G(\beta,\psi,T)\right|^2$ is (up to sign) an eigenvalue of $I\circ I$, but there may be no such eigenvalue when $\beta^2=1$. Instead, it will turn out that $\left|g^G(\beta,\psi,T)\right|^2$ equals the smallest magnitude of an eigenvalue of $I\circ I$ even when $\beta^2=1$, which can be checked by comparing the computations of \Cref{sec:qcombo,sec:gsum}.
\end{remark}